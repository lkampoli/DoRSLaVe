
# Transformers {visibility="uncounted"}

::: {.content-visible unless-format="revealjs"}
Transformers are a special type of neural networks that are proven to be highly effective in NLP tasks. They can capture long-run dependencies in the sequential data that is useful for generating predictions with contextual meaning. It makes use of the _self-attention_ mechanism which studies all inputs in the sequence together, tries to understand the dependencies among them, and then utilizes the information about long-run dependencies to predict the output sequence.
:::

## Transformer architecture

> GPT makes use of a mechanism known as attention, which removes the need for recurrent layers (e.g., LSTMs). It works like an information retrieval system, utilizing queries, keys, and values to decide how much information it wants to extract from each input token.
>
>Attention heads can be grouped together to form what is known as a multihead attention layer. These are then wrapped up inside a Transformer block, which includes layer normalization and skip connections around the attention layer. Transformer blocks can be stacked to create very deep neural networks.

Highly recommended viewing: Iulia Turk (2021), [_Transfer learning and Transformer models_](https://www.youtube.com/watch?v=LE3NfEULV6k), ML Tech Talks.

::: footer
Source: David Foster (2023), Generative Deep Learning, 2nd Edition, O'Reilly Media, Chapter 9.
:::

## ü§ó Transformers package

::: {.content-visible unless-format="revealjs"}
The following code uses transformers library from _Hugging Face_ to create a text generation pipeline using the GPT2 (Generative Pre-trained Transformer 2).
:::

```{python}
import transformers                                                                 #<1>
from transformers import pipeline                                                   #<2>
generator = pipeline(task="text-generation", model="gpt2", revision="6c0e608")      #<3>
```
1. Imports the `transformers` library
2. Imports the class `pipeline`
3. Creates a pipeline object called the `generator`, whose task would be to generate text, using the pretrained model GPT2. `revision="6c0e608"` specifies the specific revision of the model to refer

```{python}
transformers.set_seed(1)                                                                #<1>
print(generator("It's the holidays so I'm going to enjoy")[0]["generated_text"])        #<2>
```

1. Sets the seed for reproducibility
2. Applies the `generator` object to generate a text based on the input _It's the holidays so I'm going to enjoy_. The result from genrator would be a list of generated texts. To select the first output sequence hence, we pass the command `[0]["generated_text"]`

::: {.content-visible unless-format="revealjs"}
We can try the same code with a different seed value, and it would give a very different output.
:::

```{python}
transformers.set_seed(2)
print(generator("It's the holidays so I'm going to enjoy")[0]["generated_text"])
```

## Reading the course profile

::: {.content-visible unless-format="revealjs"}
Another application of pipeline is the ability to generate texts in the answer format. The following is an example of how a pretrained model can be used to answer questions by relating it to a body of text information (context).
:::

```{python}
context = """
StoryWall Formative Discussions: An initial StoryWall, worth 2%, is due by noon on June 3. The following StoryWalls are worth 4% each (taking the best 7 of 9) and are due at noon on the following dates:
The project will be submitted in stages: draft due at noon on July 1 (10%), recorded presentation due at noon on July 22 (15%), final report due at noon on August 1 (15%).

As a student at UNSW you are expected to display academic integrity in your work and interactions. Where a student breaches the UNSW Student Code with respect to academic integrity, the University may take disciplinary action under the Student Misconduct Procedure. To assure academic integrity, you may be required to demonstrate reasoning, research and the process of constructing work submitted for assessment.
To assist you in understanding what academic integrity means, and how to ensure that you do comply with the UNSW Student Code, it is strongly recommended that you complete the Working with Academic Integrity module before submitting your first assessment task. It is a free, online self-paced Moodle module that should take about one hour to complete.

StoryWall (30%)

The StoryWall format will be used for small weekly questions. Each week of questions will be released on a Monday, and most of them will be due the following Monday at midday (see assessment table for exact dates). Students will upload their responses to the question sets, and give comments on another student's submission. Each week will be worth 4%, and the grading is pass/fail, with the best 7 of 9 being counted. The first week's basic 'introduction' StoryWall post is counted separately and is worth 2%.

Project (40%)

Over the term, students will complete an individual project. There will be a selection of deep learning topics to choose from (this will be outlined during Week 1).

The deliverables for the project will include: a draft/progress report mid-way through the term, a presentation (recorded), a final report including a written summary of the project and the relevant Python code (Jupyter notebook).

Exam (30%)

The exam will test the concepts presented in the lectures. For example, students will be expected to: provide definitions for various deep learning terminology, suggest neural network designs to solve risk and actuarial problems, give advice to mock deep learning engineers whose projects have hit common roadblocks, find/explain common bugs in deep learning Python code.
"""
```

## Question answering 
```{python}
qa = pipeline("question-answering", model="distilbert-base-cased-distilled-squad", revision="626af31")      #<1>
```

1. Creates a _question and answer_ style pipeline object by referring to the pretrained model pre-trained _DistilBERT_ model (fine-tuned on the SQuAD: Stanford Question Answering Dataset) with revision `626af31`

```{python}
qa(question="What weight is the exam?", context=context)                    #<1>
```

1. Answers the questions _What weight is the exam_ given the context specified

```{python}
qa(question="What topics are in the exam?", context=context)
```

```{python}
qa(question="When is the presentation due?", context=context)
```

```{python}
qa(question="How many StoryWall tasks are there?", context=context)
```

## ChatGPT is Transformer + RLHF

> At the time of writing, there is no official paper that describes how ChatGPT works in detail, but from the official blog post we know that it uses a technique called reinforcement learning from human feedback (RLHF) to fine-tune the GPT-3.5 model.

> While ChatGPT still has many limitations (such as sometimes ‚Äúhallucinating‚Äù factually incorrect information), it is a powerful example of how Transformers can be used to build generative models that can produce complex, long-ranging, and novel output that is often indistinguishable from human-generated text. The progress made thus far by models like ChatGPT serves as a testament to the potential of AI and its transformative impact on the world.

::: footer
Source: David Foster (2023), Generative Deep Learning, 2nd Edition, O'Reilly Media, Chapter 9.
:::

## ChatGPT internals

![It uses a fair bit of human feedback](ChatGPT-Diagram.png)

::: footer
Source: [OpenAI blog](https://openai.com/blog/chatgpt).
:::

## Recommended reading {.smaller}

- The Verge (2022), [The Great Fiction of AI: The strange world of high-speed semi-automated genre fiction](https://www.theverge.com/c/23194235/ai-fiction-writing-amazon-kindle-sudowrite-jasper)
- Vaswani et al. (2017), [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), NeurIPS
- Bommasani et al. (2021), [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)
- Gary Marcus (2022), [Deep Learning Is Hitting a Wall](https://nautil.us/deep-learning-is-hitting-a-wall-14467/), Nautilus article
- Super Data Science episode 564, [Clem Delangue on Hugging Face and Transformers](https://podcasts.apple.com/au/podcast/super-data-science/id1163599059?i=1000556643700)
- Super Data Science episode 559, [GPT-3 for Natural Language Processing](https://podcasts.apple.com/au/podcast/super-data-science/id1163599059?i=1000554847681)
- Computerphile (2019), [AI Language Models & Transformers](https://youtu.be/rURRYI66E54) (20m)
- Computerphile (2020), [GPT3: An Even Bigger Language Model](https://youtu.be/_8yVOC4ciXc) (25m)
- Nicholas Renotte (2021), [AI Blog Post Summarization with Hugging Face Transformers...](https://youtu.be/JctmnczWg0U) (33m)
- Seattle Applied Deep Learning (2019), [LSTM is dead. Long Live Transformers!](https://youtu.be/S27pHKBEp30) (28m)
