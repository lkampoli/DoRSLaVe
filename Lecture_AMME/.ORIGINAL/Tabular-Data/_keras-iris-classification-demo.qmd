# Classification {visibility="uncounted"}

```{python}
#| echo: false
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

pandas.options.display.max_rows = 4
```

## Iris dataset

```{python}
from sklearn.datasets import load_iris
iris = load_iris()
names = ["SepalLength", "SepalWidth", "PetalLength", "PetalWidth"]
features = pd.DataFrame(iris.data, columns=names)
features
```

## Target variable

::: columns
::: column

```{python}
iris.target_names
```

```{python}
iris.target[:8]
```

```{python}
target = iris.target
target = target.reshape(-1, 1)
target[:8]
```
:::
::: column
```{python}
classes, counts = np.unique(
        target,
        return_counts=True
)
print(classes)
print(counts)
```

```{python}
iris.target_names[
  target[[0, 30, 60]]
]
```
:::
:::

```{python}
#| echo: false
pandas.options.display.max_rows = 6
```

## Split the data into train and test {.smaller}

```{python}
X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=24)
X_train
```

```{python}
X_test.shape, y_test.shape
```

## A basic classifier network

![A basic network for classifying into three categories.](basic-classifier-network.png)

::: footer
Source: Marcus Lautier (2022).
:::

::: {.content-visible unless-format="revealjs"}
Since the task is a classification problem, we use `softmax` activation function. The softmax function takes in the input and returns a probability vector, which tells us about the probability of a data point belonging to a certain class.
:::
## Create a classifier model

```{python}
NUM_FEATURES = len(features.columns)
NUM_CATS = len(np.unique(target))

print("Number of features:", NUM_FEATURES)
print("Number of categories:", NUM_CATS)
```

Make a function to return a Keras model:
```{python}
def build_model(seed=42):
    random.seed(seed)
    return Sequential([
        Dense(30, activation="relu"),
        Dense(NUM_CATS, activation="softmax")
    ])
```

## Fit the model

```{python}
model = build_model()
model.compile("adam", "sparse_categorical_crossentropy")

model.fit(X_train, y_train, epochs=5, verbose=2);
```
::: {.content-visible unless-format="revealjs"}
Since the problem at hand is a classification problem, we define the optimizer and loss function accordingly. Optimizer is `adam` and the loss function is `sparse_categorical_crossentropy`. If the response variable represents the category directly using an integer (i.e. if the response variable is not one-hot encoded), we must use `sparse_categorical_crossentropy`. If the response variable (y label) is already one-hot encoded we can use `categorical_crossentropy`. 
:::

## Track accuracy as the model trains

```{python}
model = build_model()
model.compile("adam", "sparse_categorical_crossentropy", metrics=["accuracy"])
model.fit(X_train, y_train, epochs=5, verbose=2);
```
::: {.content-visible unless-format="revealjs"}
We can also specify which loss metric to monitor in assessing the performance during the training. The metric that is usually used in classification tasks is `accuracy`, which tracks the fraction of all predictions which identified the class accurately. The metrics are not used for optimizing. They are only used to keep track of how well the model is performing during the optimization.  By setting `verbose=2`, we are printing the progress during training, and we can see how the loss is reducing and accuracy is improving.
:::

## Run a long fit
::: {.content-visible unless-format="revealjs"}
Run the model training for 500 epochs.
:::
```{python}

model = build_model()
model.compile("adam", "sparse_categorical_crossentropy", \
        metrics=["accuracy"])
%time hist = model.fit(X_train, y_train, epochs=500, \
        validation_split=0.25, verbose=False)
```

Evaluation now returns both _loss_ and _accuracy_.
```{python}
model.evaluate(X_test, y_test, verbose=False)
```

## Add early stopping

```{python}
model = build_model()                                                   #<1>
model.compile("adam", "sparse_categorical_crossentropy", \
        metrics=["accuracy"])                                           #<2>

es = EarlyStopping(restore_best_weights=True, patience=50,              #<3>
        monitor="val_accuracy")                                         
%time hist_es = model.fit(X_train, y_train, epochs=500, \
        validation_split=0.25, callbacks=[es], verbose=False);          #<4>

print(f"Stopped after {len(hist_es.history['loss'])} epochs.")
```
::: {.content-visible unless-format="revealjs"}
1. Defines a new model with the same architecture as `model_build` which is already constructed
2. Compiles the model with optimizer, loss function and metric
3. Defines the early stopping object as usual, with one slight change. The code is specified to activate the early stopping by monitoring the validation accuracy (`val_accuracy`), not the loss. 
4. Fits the model
:::

Evaluation on test set:
```{python}
model.evaluate(X_test, y_test, verbose=False)
```

## Fitting metrics

::: columns
::: column
```{python}
#| echo: false
matplotlib.pyplot.rcParams["figure.figsize"] = (2.5, 2.95)
plt.subplot(2, 1, 1)
plt.plot(hist.history["loss"])
plt.plot(hist.history["val_loss"])
plt.title("Loss")
plt.legend(["Training", "Validation"])

plt.subplot(2, 1, 2)
plt.plot(hist_es.history["loss"])
plt.plot(hist_es.history["val_loss"])
plt.xlabel("Epoch");
```
:::
::: column
```{python}
#| echo: false
matplotlib.pyplot.rcParams["figure.figsize"] = (2.5, 3.25)
plt.subplot(2, 1, 1)
plt.plot(hist.history["accuracy"])
plt.plot(hist.history["val_accuracy"])
plt.title("Accuracy")

plt.subplot(2, 1, 2)
plt.plot(hist_es.history["accuracy"])
plt.plot(hist_es.history["val_accuracy"])
plt.xlabel("Epoch");
```
:::
:::

```{python}
#| echo: false
set_rectangular_figures()
```

::: {.content-visible unless-format="revealjs"}
Left hand side plots show how loss behaved without and with early stopping. Right hand side plots show how accuracy performed without and with early stopping.
:::
## What is the softmax activation?

It creates a "probability" vector: $\text{Softmax}(\boldsymbol{x}) = \frac{\mathrm{e}^x_i}{\sum_j \mathrm{e}^x_j} \,.$

In NumPy:
```{python}
out = np.array([5, -1, 6])
(np.exp(out) / np.exp(out).sum()).round(3)
```

In Keras:
```{python}
out = keras.ops.convert_to_tensor([[5.0, -1.0, 6.0]])
keras.ops.round(keras.ops.softmax(out), 3)
```

## Prediction using classifiers

```{python}
y_test[:4]
```
::: {.content-visible unless-format="revealjs"}
The response variable `y` is an array of numeric integers, each representing a class to which the data belongs. However, the `model.predict()` function returns an array with probabilities not an array with integers. The array displays the probabilities of belonging to each category.
:::
```{python}
y_pred = model.predict(X_test.head(4), verbose=0)
y_pred
```
::: {.content-visible unless-format="revealjs"}
Using `np.argmax()` which returns index of the maximum value in an array, we can obtain the predicted class.
:::

```{python}
# Add 'keepdims=True' to get a column vector.
np.argmax(y_pred, axis=1)
```

```{python}
iris.target_names[np.argmax(y_pred, axis=1)]
```

## Cross-entropy loss: ELI5

::: {.content-visible unless-format="revealjs"}
::: columns
::: column

{{< video https://www.youtube.com/embed/6ArSys5qHAU aspect-ratio="1x1" >}}

:::
::: column

{{< video https://www.youtube.com/embed/xBEh66V9gZo aspect-ratio="1x1" >}}

:::
:::
:::

::: {.content-hidden unless-format="revealjs"}
::: columns
::: column

{{< video https://www.youtube.com/embed/6ArSys5qHAU width="560" height="315" >}}

:::
::: column

{{< video https://www.youtube.com/embed/xBEh66V9gZo width="560" height="315" >}}

:::
:::
:::


## Why use cross-entropy loss?

```{python}
p = np.linspace(0, 1, 100)
plt.plot(p, (1 - p) ** 2)
plt.plot(p, -np.log(p))
plt.legend(["MSE", "Cross-entropy"]);
```

::: {.content-visible unless-format="revealjs"}
The above plot shows how MSE and cross-entropy penalize wrong predictions. The x-axis indicates the severity of misclassification. Suppose the neural network predicted that there is near-zero probability of an observation being in class "1" when the actual class is "1". This represents a strong misclassification. The above graph shows how MSE does not impose heavy penalties for the misclassifications near zero. It displays a linear increment across the severity of misclassification. On the other hand, cross-entropy penalises bad predictions strongly. Also, the misclassification penalty grows exponentially. This makes cross entropy more suitable.
:::

## One-hot encoding {data-visibility="uncounted"}

```{python}
from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder(sparse_output=False)

y_train_oh = enc.fit_transform(y_train)
y_test_oh = enc.transform(y_test)
```

::: columns
::: column
```{python}
y_train[:5]
```
:::
::: column
```{python}
y_train_oh[:5]
```
:::
:::

## Classifier given one-hot outputs

Create the model (_new loss function_):
```{python}
#| code-line-numbers: "|3"
model = build_model()
model.compile("adam", "categorical_crossentropy", \
    metrics=["accuracy"])
```

Fit the model (_new target variables_):
```{python}
model.fit(X_train, y_train_oh, epochs=100, verbose=False);
```

Evaluate the model (_new target variables_):
```{python}
model.evaluate(X_test, y_test_oh, verbose=False)
```
