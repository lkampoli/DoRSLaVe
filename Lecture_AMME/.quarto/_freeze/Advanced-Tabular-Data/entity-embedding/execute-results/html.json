{
  "hash": "659603934dd39de66df93879f1d63779",
  "result": {
    "markdown": "---\ntitle: Entity Embedding\n---\n\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the package imports\"}\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n```\n:::\n\n\n:::\n\n# Entity Embedding {visibility=\"uncounted\"}\n\n## Continuing on the French motor dataset example\n\nDownload the dataset if we don't have it already.\n\n::: {.cell output-location='slide' execution_count=3}\n``` {.python .cell-code}\nfrom pathlib import Path                                      #<1>\nfrom sklearn.datasets import fetch_openml                     #<2>\n\nif not Path(\"french-motor.csv\").exists():                     #<3>\n    freq = fetch_openml(data_id=41214, as_frame=True).frame   #<4>\n    freq.to_csv(\"french-motor.csv\", index=False)              #<5>\nelse:\n    freq = pd.read_csv(\"french-motor.csv\")                    #<6>\n\nfreq\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IDpol</th>\n      <th>ClaimNb</th>\n      <th>Exposure</th>\n      <th>Area</th>\n      <th>VehPower</th>\n      <th>VehAge</th>\n      <th>DrivAge</th>\n      <th>BonusMalus</th>\n      <th>VehBrand</th>\n      <th>VehGas</th>\n      <th>Density</th>\n      <th>Region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.10000</td>\n      <td>D</td>\n      <td>5</td>\n      <td>0</td>\n      <td>55</td>\n      <td>50</td>\n      <td>B12</td>\n      <td>'Regular'</td>\n      <td>1217</td>\n      <td>R82</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0.77000</td>\n      <td>D</td>\n      <td>5</td>\n      <td>0</td>\n      <td>55</td>\n      <td>50</td>\n      <td>B12</td>\n      <td>'Regular'</td>\n      <td>1217</td>\n      <td>R82</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>678011</th>\n      <td>6114329.0</td>\n      <td>0</td>\n      <td>0.00274</td>\n      <td>B</td>\n      <td>4</td>\n      <td>0</td>\n      <td>60</td>\n      <td>50</td>\n      <td>B12</td>\n      <td>'Regular'</td>\n      <td>95</td>\n      <td>R26</td>\n    </tr>\n    <tr>\n      <th>678012</th>\n      <td>6114330.0</td>\n      <td>0</td>\n      <td>0.00274</td>\n      <td>B</td>\n      <td>7</td>\n      <td>6</td>\n      <td>29</td>\n      <td>54</td>\n      <td>B12</td>\n      <td>'Diesel'</td>\n      <td>65</td>\n      <td>R72</td>\n    </tr>\n  </tbody>\n</table>\n<p>678013 rows × 12 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: footer\nSource: Nell et al. (2020), [Case Study: French Motor Third-Party Liability Claims](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764), SSRN.\n:::\n\n\n## Data dictionary {.smaller}\n\n::: columns\n::: column\n- `IDpol`: policy number (unique identifier)\n- `ClaimNb`: number of claims on the given policy\n- `Exposure`: total exposure in yearly units\n- `Area`: area code (categorical, ordinal)\n- `VehPower`: power of the car (categorical, ordinal)\n- `VehAge`: age of the car in years\n- `DrivAge`: age of the (most common) driver in years\n:::\n::: column\n- `BonusMalus`: bonus-malus level between 50 and 230 (with reference level 100)\n- `VehBrand`: car brand (categorical, nominal)\n- `VehGas`: diesel or regular fuel car (binary)\n- `Density`: density of inhabitants per km^2^ in the city of the living place of the driver\n- `Region`: regions in France (prior to 2016)\n:::\n:::\n\n::: footer\nSource: Nell et al. (2020), [Case Study: French Motor Third-Party Liability Claims](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764), SSRN.\n:::\n\n## The model\n\nHave $\\{ (\\mathbf{x}_i, y_i) \\}_{i=1, \\dots, n}$ for $\\mathbf{x}_i \\in \\mathbb{R}^{47}$ and $y_i \\in \\mathbb{N}_0$.\n\nAssume the distribution\n$$\nY_i \\sim \\mathsf{Poisson}(\\lambda(\\mathbf{x}_i))\n$$\n\nWe have $\\mathbb{E} Y_i = \\lambda(\\mathbf{x}_i)$. \nThe NN takes $\\mathbf{x}_i$ & predicts $\\mathbb{E} Y_i$.\n\n::: {.callout-note}\nFor insurance, _this is a bit weird_.\nThe exposures are different for each policy.\n\n$\\lambda(\\mathbf{x}_i)$ is the expected number of claims for the duration of policy $i$'s contract.\n\nNormally, $\\text{Exposure}_i \\not\\in \\mathbf{x}_i$, and $\\lambda(\\mathbf{x}_i)$ is the expected rate _per year_, then\n$$\nY_i \\sim \\mathsf{Poisson}(\\text{Exposure}_i \\times \\lambda(\\mathbf{x}_i)).\n$$\n:::\n\n## Where are things defined?\n\nIn Keras, string options are used for convenience to reference specific functions or settings.\n\n::: {.content-visible unless-format=\"revealjs\"}\nMeaning that setting `activation=\"relu\"` (with in strings) is same as setting `activation=relu` after bringing in the `relu` function from `keras.activations`. \n:::\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nmodel = Sequential([\n    Dense(30, activation=\"relu\"),\n    Dense(1, activation=\"exponential\")\n])\n```\n:::\n\n\nis the same as\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom keras.activations import relu, exponential\n\nmodel = Sequential([\n    Dense(30, activation=relu),\n    Dense(1, activation=exponential)\n])\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nx = [-1.0, 0.0, 1.0]\nprint(relu(x))\nprint(exponential(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([0. 0. 1.], shape=(3,), dtype=float32)\ntf.Tensor([0.37 1.   2.72], shape=(3,), dtype=float32)\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can see how `relu` function gives out _x_ when _x_  is non-negative, and gives out 0 when _x_ is negative. `exponential` function, takes in _x_ and gives out the _exp(x)_.\n:::\n\n## String arguments to `.compile`\n\nWhen we run\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nmodel.compile(optimizer=\"adam\", loss=\"poisson\")\n```\n:::\n\n\nit is equivalent to\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom keras.losses import poisson\nfrom keras.optimizers import Adam\n\nmodel.compile(optimizer=Adam(), loss=poisson)\n```\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nThis is akin to specifying the activation function directly. Setting `optimizer=\"adam\"` and `loss=\"poisson\"` as strings is equivalent to using `optimizer=Adam()` and `loss=poisson` after importing `Adam` from `keras.optimizers` and `poisson` from `keras.losses`. Another important thing to note here is that, the loss function is no longer `mse`. Since we assume a Poisson distribution for the target variable, and the goal is to optimise the algorithm for count data, Poisson loss is more appropriate.\n:::\n\nWhy do this manually? To adjust the object:\n\n::: {.content-visible unless-format=\"revealjs\"}\nOne of the main reasons why we would want to bring in the functions from the libraries (as opposed to using strings) is because it allows us to control the hyper-parameters of the object. For instance, in the example below, we can see how we set the `learning_rate` to a specific value. `learning_rate` is an important hyper-parameter in neural network training because it controls the pace at which weights of the neural networks are updated. Too small learning rates can result in slower learning, hence, longer training time. Too large learning rates lead to large steps in weights updates, hence, might miss the optimal solution.\n:::\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\noptimizer = Adam(learning_rate=0.01)\nmodel.compile(optimizer=optimizer, loss=\"poisson\")\n```\n:::\n\n\nor to get help.\n\n## Keras' \"poisson\" loss\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nhelp(keras.losses.poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHelp on function poisson in module keras.src.losses.losses:\n\npoisson(y_true, y_pred)\n    Computes the Poisson loss between y_true and y_pred.\n    \n    Formula:\n    \n    ```python\n    loss = y_pred - y_true * log(y_pred)\n    ```\n    \n    Args:\n        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n    \n    Returns:\n        Poisson loss values with shape = `[batch_size, d0, .. dN-1]`.\n    \n    Example:\n    \n    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n    >>> y_pred = np.random.random(size=(2, 3))\n    >>> loss = keras.losses.poisson(y_true, y_pred)\n    >>> assert loss.shape == (2,)\n    >>> y_pred = y_pred + 1e-7\n    >>> assert np.allclose(\n    ...     loss, np.mean(y_pred - y_true * np.log(y_pred), axis=-1),\n    ...     atol=1e-5)\n\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nUsing the help function in this case provides information about the Poisson loss function in the `keras.losses library`. It shows that how `poisson` loss is calculated, by taking two inputs, (i) actual values and (ii) predicted values.\n:::\n\n## Subsample and split\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfreq = freq.drop(\"IDpol\", axis=1).head(25_000)                          #<1>\n\nX_train, X_test, y_train, y_test = train_test_split(                    #<2>\n  freq.drop(\"ClaimNb\", axis=1), freq[\"ClaimNb\"], random_state=2023)     #<2>\n\n# Reset each index to start at 0 again.\nX_train = X_train.reset_index(drop=True)                                #<3>\nX_test = X_test.reset_index(drop=True)                                  #<3>\n```\n:::\n\n\n## What values do we see in the data?\n\n::: {layout-ncol=2 layout-nrow=2}\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nX_train[\"Area\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nArea\nC    5507\nD    4113\n     ... \nB    2359\nF     475\nName: count, Length: 6, dtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nX_train[\"VehBrand\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nVehBrand\nB1     5069\nB2     4838\n       ... \nB11     284\nB14     136\nName: count, Length: 11, dtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nX_train[\"VehGas\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nVehGas\n'Regular'    10773\n'Diesel'      7977\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nX_train[\"Region\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nRegion\nR24    6498\nR82    2119\n       ... \nR42      55\nR43      26\nName: count, Length: 22, dtype: int64\n```\n:::\n:::\n\n\n:::\n\n## Preprocess ordinal & continuous\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.compose import make_column_transformer\n\nct = make_column_transformer(\n  (OrdinalEncoder(), [\"Area\", \"VehGas\"]),\n  (\"drop\", [\"VehBrand\", \"Region\"]),\n  remainder=StandardScaler(),\n  verbose_feature_names_out=False\n)\nX_train_ct = ct.fit_transform(X_train)\n```\n:::\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nX_train.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Exposure</th>\n      <th>Area</th>\n      <th>VehPower</th>\n      <th>VehAge</th>\n      <th>DrivAge</th>\n      <th>BonusMalus</th>\n      <th>VehBrand</th>\n      <th>VehGas</th>\n      <th>Density</th>\n      <th>Region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.00</td>\n      <td>C</td>\n      <td>6</td>\n      <td>2</td>\n      <td>66</td>\n      <td>50</td>\n      <td>B2</td>\n      <td>'Diesel'</td>\n      <td>124</td>\n      <td>R24</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.36</td>\n      <td>C</td>\n      <td>4</td>\n      <td>10</td>\n      <td>22</td>\n      <td>100</td>\n      <td>B1</td>\n      <td>'Regular'</td>\n      <td>377</td>\n      <td>R93</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02</td>\n      <td>E</td>\n      <td>12</td>\n      <td>8</td>\n      <td>44</td>\n      <td>60</td>\n      <td>B3</td>\n      <td>'Regular'</td>\n      <td>5628</td>\n      <td>R11</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nX_train_ct.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>VehGas</th>\n      <th>Exposure</th>\n      <th>VehPower</th>\n      <th>VehAge</th>\n      <th>DrivAge</th>\n      <th>BonusMalus</th>\n      <th>Density</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.126979</td>\n      <td>-0.165005</td>\n      <td>-0.844589</td>\n      <td>1.451036</td>\n      <td>-0.637179</td>\n      <td>-0.366980</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>-0.590896</td>\n      <td>-1.228181</td>\n      <td>0.586255</td>\n      <td>-1.548692</td>\n      <td>2.303010</td>\n      <td>-0.302700</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>-1.503517</td>\n      <td>3.024524</td>\n      <td>0.228544</td>\n      <td>-0.048828</td>\n      <td>-0.049141</td>\n      <td>1.031432</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n:::\n\n# Categorical Variables & Entity Embeddings {visibility=\"uncounted\"}\n\n## Region column\n\n![French Administrative Regions](french-regions.png)\n\n::: footer\nSource: Nell et al. (2020), [Case Study: French Motor Third-Party Liability Claims](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764), SSRN.\n:::\n\n## One-hot encoding\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\noe = OneHotEncoder(sparse_output=False)\nX_train_oh = oe.fit_transform(X_train[[\"Region\"]])\nX_test_oh = oe.transform(X_test[[\"Region\"]])\nprint(list(X_train[\"Region\"][:5]))\nX_train_oh.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['R24', 'R93', 'R11', 'R42', 'R24']\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region_R11</th>\n      <th>Region_R21</th>\n      <th>Region_R22</th>\n      <th>Region_R23</th>\n      <th>Region_R24</th>\n      <th>Region_R25</th>\n      <th>Region_R26</th>\n      <th>Region_R31</th>\n      <th>Region_R41</th>\n      <th>Region_R42</th>\n      <th>...</th>\n      <th>Region_R53</th>\n      <th>Region_R54</th>\n      <th>Region_R72</th>\n      <th>Region_R73</th>\n      <th>Region_R74</th>\n      <th>Region_R82</th>\n      <th>Region_R83</th>\n      <th>Region_R91</th>\n      <th>Region_R93</th>\n      <th>Region_R94</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nOne hot encoding is a way to assign numerical values to nominal variables. One hot encoding is different from ordinal encoding in the way in which it transforms the data. Ordinal encoding assigns a numerical integer to each unique category of the data column and returns one integer column. In contrast, one hot encoding returns a binary vector for each unique category. As a result, what we get from one hot encoding is not a single column vector, but a matrix with number of columns equal to the number of unique categories in that nominal data column.\n:::\n\n## Train on one-hot inputs\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nnum_regions = len(oe.categories_[0])                            #<1>\n\nrandom.seed(12)\nmodel = Sequential([                                            #<2> \n  Dense(2, input_dim=num_regions),\n  Dense(1, activation=\"exponential\")\n])\n\nmodel.compile(optimizer=\"adam\", loss=\"poisson\")                #<3>  \n\nes = EarlyStopping(verbose=True)                               #<4> \nhist = model.fit(X_train_oh, y_train, epochs=100, verbose=0,   #<5> \n    validation_split=0.2, callbacks=[es])                       \nhist.history[\"val_loss\"][-1]                                   #<6> \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/unimelb.edu.au/lcampoli/miniconda3/envs/AMME/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 12: early stopping\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n0.7526934146881104\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe above code shows how we can train a neural network using only the one-hot encoded variables.  The example is similar to the case of training neural networks for ordinal encoding. \n1. Computes the number of unique categories in the encoded column and store it in `num_regions`\n2. Constructs the neural network. This time, it is a neural network with 1 hidden layer and 1 output layer. `Dense(2, input_dim=num_regions)` takes in an input matrix of with columns = `num_regions` and transofrmas it down to an output with 2 neurons\nSteps 3-6 is similar to what we saw during training with ordinal encoded variables.\n:::\n\n## Consider the first layer {.smaller}\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nevery_category = pd.DataFrame(np.eye(num_regions), columns=oe.categories_[0])\nevery_category.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R11</th>\n      <th>R21</th>\n      <th>R22</th>\n      <th>R23</th>\n      <th>R24</th>\n      <th>R25</th>\n      <th>R26</th>\n      <th>R31</th>\n      <th>R41</th>\n      <th>R42</th>\n      <th>...</th>\n      <th>R53</th>\n      <th>R54</th>\n      <th>R72</th>\n      <th>R73</th>\n      <th>R74</th>\n      <th>R82</th>\n      <th>R83</th>\n      <th>R91</th>\n      <th>R93</th>\n      <th>R94</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 22 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# Put this through the first layer of the model\nX = every_category.to_numpy()                       #<1>\nmodel.layers[0](X)                                  #<2>\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n<tf.Tensor: shape=(22, 2), dtype=float32, numpy=\narray([[-0.21, -0.14],\n       [ 0.21, -0.17],\n       [-0.22,  0.1 ],\n       [-0.83,  0.1 ],\n       [-0.01, -0.66],\n       [-0.65, -0.13],\n       [-0.36, -0.41],\n       [ 0.21, -0.03],\n       [-0.93, -0.57],\n       [ 0.2 , -0.41],\n       [-0.43, -0.21],\n       [-1.13, -0.33],\n       [ 0.17, -0.68],\n       [-0.88, -0.55],\n       [-0.13,  0.05],\n       [ 0.11,  0.  ],\n       [-0.46, -0.38],\n       [-0.62, -0.37],\n       [-0.19, -0.28],\n       [-0.22,  0.15],\n       [ 0.3 , -0.16],\n       [-0.28,  0.36]], dtype=float32)>\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can extract each layer separately from a trained neural network and observe its output given a specific input. \n1. Converts the dataframe to a numpy array\n2. Takes out the first layer and feeds in the numpy array _X_. This returns an array with 2 columns\n:::\n## The first layer\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nlayer = model.layers[0]                     #<1>\nW, b = layer.get_weights()                  #<2>\nX.shape, W.shape, b.shape                   #<3>\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n((22, 22), (22, 2), (2,))\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can also extract the layer, get its wieghts and compute manually. \n1. Extracts the layer\n2. Gets the weights and biases and stores the weights in _W_ and biases in _b_\n3. Returns the shapes of the matrices\n:::\n\n::: columns\n::: column\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nX @ W + b\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\narray([[-0.21, -0.14],\n       [ 0.21, -0.17],\n       [-0.22,  0.1 ],\n       [-0.83,  0.1 ],\n       [-0.01, -0.66],\n       [-0.65, -0.13],\n       [-0.36, -0.41],\n       [ 0.21, -0.03],\n       [-0.93, -0.57],\n       [ 0.2 , -0.41],\n       [-0.43, -0.21],\n       [-1.13, -0.33],\n       [ 0.17, -0.68],\n       [-0.88, -0.55],\n       [-0.13,  0.05],\n       [ 0.11,  0.  ],\n       [-0.46, -0.38],\n       [-0.62, -0.37],\n       [-0.19, -0.28],\n       [-0.22,  0.15],\n       [ 0.3 , -0.16],\n       [-0.28,  0.36]])\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nW + b\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray([[-0.21, -0.14],\n       [ 0.21, -0.17],\n       [-0.22,  0.1 ],\n       [-0.83,  0.1 ],\n       [-0.01, -0.66],\n       [-0.65, -0.13],\n       [-0.36, -0.41],\n       [ 0.21, -0.03],\n       [-0.93, -0.57],\n       [ 0.2 , -0.41],\n       [-0.43, -0.21],\n       [-1.13, -0.33],\n       [ 0.17, -0.68],\n       [-0.88, -0.55],\n       [-0.13,  0.05],\n       [ 0.11,  0.  ],\n       [-0.46, -0.38],\n       [-0.62, -0.37],\n       [-0.19, -0.28],\n       [-0.22,  0.15],\n       [ 0.3 , -0.16],\n       [-0.28,  0.36]], dtype=float32)\n```\n:::\n:::\n\n\n:::\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe above codes manually compute and returns the same answers as before.\n:::\n## Just a look-up operation\n\n::: columns\n::: column\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ndisplay(list(oe.categories_[0]))\n```\n\n::: {.cell-output .cell-output-display}\n```\n['R11',\n 'R21',\n 'R22',\n 'R23',\n 'R24',\n 'R25',\n 'R26',\n 'R31',\n 'R41',\n 'R42',\n 'R43',\n 'R52',\n 'R53',\n 'R54',\n 'R72',\n 'R73',\n 'R74',\n 'R82',\n 'R83',\n 'R91',\n 'R93',\n 'R94']\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nW + b\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\narray([[-0.21, -0.14],\n       [ 0.21, -0.17],\n       [-0.22,  0.1 ],\n       [-0.83,  0.1 ],\n       [-0.01, -0.66],\n       [-0.65, -0.13],\n       [-0.36, -0.41],\n       [ 0.21, -0.03],\n       [-0.93, -0.57],\n       [ 0.2 , -0.41],\n       [-0.43, -0.21],\n       [-1.13, -0.33],\n       [ 0.17, -0.68],\n       [-0.88, -0.55],\n       [-0.13,  0.05],\n       [ 0.11,  0.  ],\n       [-0.46, -0.38],\n       [-0.62, -0.37],\n       [-0.19, -0.28],\n       [-0.22,  0.15],\n       [ 0.3 , -0.16],\n       [-0.28,  0.36]], dtype=float32)\n```\n:::\n:::\n\n\n:::\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe above outputs show that the neural network thinks the best way to represent \"R11\" for this particular problem is using the vector [-0.2, -0.12]. \n:::\n## Turn the region into an index\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\noe = OrdinalEncoder()\nX_train_reg = oe.fit_transform(X_train[[\"Region\"]])\nX_test_reg = oe.transform(X_test[[\"Region\"]])\n\nfor i, reg in enumerate(oe.categories_[0][:3]):\n  print(f\"The Region value {reg} gets turned into {i}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Region value R11 gets turned into 0.\nThe Region value R21 gets turned into 1.\nThe Region value R22 gets turned into 2.\n```\n:::\n:::\n\n\n## Embedding\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nfrom keras.layers import Embedding\nnum_regions = len(np.unique(X_train[[\"Region\"]]))\n\nrandom.seed(12)\nmodel = Sequential([\n  Embedding(input_dim=num_regions, output_dim=2),\n  Dense(1, activation=\"exponential\")\n])\n\nmodel.compile(optimizer=\"adam\", loss=\"poisson\")\n```\n:::\n\n\n## Fitting that model\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nes = EarlyStopping(verbose=True)\nhist = model.fit(X_train_reg, y_train, epochs=100, verbose=0,\n    validation_split=0.2, callbacks=[es])\nhist.history[\"val_loss\"][-1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 5: early stopping\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n0.7526668906211853\n```\n:::\n:::\n\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nmodel.layers\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n[<Embedding name=embedding, built=True>, <Dense name=dense_6, built=True>]\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nEmbedding layer can learn the optimal representation for a category of a categorical variable, during training. \nIn the above example, encoding the variable _Region_ using ordinal encoding and passing it through an embedding layer learns the optimal representation for the region during training. Ordinal encoding followed with an embedding layer is a better alternative to one-hot encoding. It is computationally less expensive (compared to generating large matrices in one-hot encoding) particularly when the number of categories is high.\n:::\n\n## Keras' Embedding Layer\n\n::: columns\n::: column\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nmodel.layers[0].get_weights()[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\narray([[-0.12, -0.11],\n       [ 0.03, -0.  ],\n       [-0.02,  0.01],\n       [-0.25, -0.14],\n       [-0.28, -0.32],\n       [-0.3 , -0.22],\n       [-0.31, -0.28],\n       [ 0.1 ,  0.07],\n       [-0.61, -0.51],\n       [-0.06, -0.12],\n       [-0.17, -0.14],\n       [-0.6 , -0.46],\n       [-0.22, -0.27],\n       [-0.59, -0.5 ],\n       [-0.  ,  0.02],\n       [ 0.07,  0.06],\n       [-0.31, -0.28],\n       [-0.4 , -0.34],\n       [-0.16, -0.15],\n       [ 0.01,  0.05],\n       [ 0.08,  0.03],\n       [ 0.08,  0.13]], dtype=float32)\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nX_train[\"Region\"].head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\n0    R24\n1    R93\n2    R11\n3    R42\nName: Region, dtype: object\n```\n:::\n:::\n\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nX_sample = X_train_reg[:4].to_numpy()\nX_sample\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\narray([[ 4.],\n       [20.],\n       [ 0.],\n       [ 9.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nenc_tensor = model.layers[0](X_sample)\nkeras.ops.convert_to_numpy(enc_tensor).squeeze()\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\narray([[-0.28, -0.32],\n       [ 0.08,  0.03],\n       [-0.12, -0.11],\n       [-0.06, -0.12]], dtype=float32)\n```\n:::\n:::\n\n\n:::\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\n\n1. Returns the weights of the Embedding layer. The function `model.layers[0].get_weights()[0]` returns a 22 $\\times$ 2 weights matrix with optimal representations for each category. Here 22 corresponds to the number of unique categories, and 2 corresponds to the size of the lower dimensional space using which we represent each category. \n2. Returns the first 4 rows of train set\n3. Converts first 4 rows to a numpy array\n4. Sends the numpy array through the Embedding layer to retrieve corresponding weights\nWe can observe how the last code returns a numpy array with representations corresponding to R24, R93, R11 and R42. \n\n:::\n\n## The learned embeddings\n\n::: {.content-visible unless-format=\"revealjs\"}\n\nIf we only have two-dimensional embeddings we can plot them.\n\n:::\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\npoints = model.layers[0].get_weights()[0]\nplt.scatter(points[:,0], points[:,1])\nfor i in range(num_regions):\n  plt.text(points[i,0]+0.01, points[i,1] , s=oe.categories_[0][i])\n```\n\n::: {.cell-output .cell-output-display}\n![](entity-embedding_files/figure-html/cell-37-output-1.png){}\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWhile it not always the case, entity embeddings can at times be meaningful instead of just being useful representations. The above figure shows how plotting the learned embeddings help reveal regions which might be similar (e.g. coastal areas, hilly areas etc.).\n:::\n\n## Entity embeddings \n\n![Embeddings will gradually improve during training.](entity-embeddings.png)\n\n::: footer\nSource: Marcus Lautier (2022).\n:::\n\n## Embeddings & other inputs\n\n::: {.content-visible unless-format=\"revealjs\"}\nOften times, we deal with both categorical and numerical variables together. The following diagram shows a recommended way of inputting numerical and categorical data in to the neural network. Numerical variables are inherently numeric hence, do not require entity embedding. On the other hand, categorical variables must undergo entity embedding to convert to number format.\n:::\n\n![Illustration of a neural network with both continuous and categorical inputs.](nn-with-entity-embedding-diagram.png)\n\nWe can't do this with Sequential models...\n\n::: footer\nSource: LotusLabs Blog, [Accurate insurance claims prediction with Deep Learning](https://www.lotuslabs.ai/accurate-insurance-claims-prediction-with-deep-learning/).\n:::\n\n# Keras' Functional API {visibility=\"uncounted\"}\n\n::: {.content-visible unless-format=\"revealjs\"}\nSequential models are easy to use and do not require many specifications, however, they cannot model complex neural network architectures. Keras Functional API approach on the other hand allows the users to build complex architectures. \n:::\n\n## Converting Sequential models\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nfrom keras.models import Model\nfrom keras.layers import Input\n```\n:::\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nrandom.seed(12)\n\nmodel = Sequential([\n  Dense(30, \"leaky_relu\"),\n  Dense(1, \"exponential\")\n])\n\nmodel.compile(\n  optimizer=\"adam\",\n  loss=\"poisson\")\n\nhist = model.fit(\n  X_train_oh, y_train,\n  epochs=1, verbose=0,\n  validation_split=0.2)\nhist.history[\"val_loss\"][-1]\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\n0.7535399198532104\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nrandom.seed(12)\n\ninputs = Input(shape=(X_train_oh.shape[1],))\nx = Dense(30, \"leaky_relu\")(inputs)\nout = Dense(1, \"exponential\")(x)\nmodel = Model(inputs, out)\n\nmodel.compile(\n  optimizer=\"adam\",\n  loss=\"poisson\")\n\nhist = model.fit(\n  X_train_oh, y_train,\n  epochs=1, verbose=0,\n  validation_split=0.2)\nhist.history[\"val_loss\"][-1]\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\n0.7535399198532104\n```\n:::\n:::\n\n\n:::\n:::\n\nSee [one-length tuples](https://pat-laub.github.io/DeepLearningMaterials/2023/Lecture-1-Artificial-Intelligence/python.html#/one-length-tuples).\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe above code shows how to construct the same neural network using sequential models and Keras functional API. There are some differences in the construction. In the functional API approach, we must specify the shape of the input layer, and explicitly define the inputs and outputs of a layer. `model = Model(inputs, out)` function specifies the input and output of the model. This manner of specifying the inputs and outputs of the model allow the user to combine several inputs (inputs which are preprocessed in different ways) to finally build the model. One example would be combining entity embedded categorical variables, and scaled numerical variables.\n:::\n\n## Wide & Deep network\n\n::: columns\n::: {.column width=\"45%\"}\n![An illustration of the wide & deep network architecture.](wide-and-deep-network.png)\n:::\n::: {.column width=\"55%\"}\nAdd a _skip connection_ from input to output layers.\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\nfrom keras.layers \\\n    import Concatenate\n\ninp = Input(shape=X_train.shape[1:])\nhidden1 = Dense(30, \"leaky_relu\")(inp)\nhidden2 = Dense(30, \"leaky_relu\")(hidden1)\nconcat = Concatenate()(\n  [inp, hidden2])\noutput = Dense(1)(concat)\nmodel = Model(\n    inputs=[inp],\n    outputs=[output])\n```\n:::\n\n\n:::\n:::\n\n::: footer\nSources: Marcus Lautier (2022) & Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Chapter 10 code snippet.\n:::\n\n## Naming the layers\n\nFor complex networks, it is often useful to give meaningul names to the layers.\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\ninput_ = Input(shape=X_train.shape[1:], name=\"input\")\nhidden1 = Dense(30, activation=\"leaky_relu\", name=\"hidden1\")(input_)\nhidden2 = Dense(30, activation=\"leaky_relu\", name=\"hidden2\")(hidden1)\nconcat = Concatenate(name=\"combined\")([input_, hidden2])\noutput = Dense(1, name=\"output\")(concat)\nmodel = Model(inputs=[input_], outputs=[output])\n```\n:::\n\n\n## Inspecting a complex model\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\nfrom keras.utils import plot_model\n```\n:::\n\n\n::: columns\n::: {.column width=\"30%\"}\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nplot_model(model, show_layer_names=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n![](entity-embedding_files/figure-html/cell-44-output-1.png){}\n:::\n:::\n\n\n:::\n::: {.column width=\"70%\"}\n::: {.smaller}\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\nmodel.summary(line_length=75)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">   Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n├─────────────────────┼───────────────────┼───────────┼───────────────────┤\n│ hidden1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼───────────┼───────────────────┤\n│ hidden2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">930</span> │ hidden1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼───────────┼───────────────────┤\n│ combined            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │           │ hidden2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼───────────┼───────────────────┤\n│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span> │ combined[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n└─────────────────────┴───────────────────┴───────────┴───────────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,301</span> (5.08 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,301</span> (5.08 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n```\n:::\n:::\n\n\n:::\n:::\n:::\n\n# French Motor Dataset with Embeddings {visibility=\"uncounted\"}\n\n## The desired architecture\n\n![Illustration of a neural network with both continuous and categorical inputs.](nn-with-entity-embedding-diagram.png)\n\n::: footer\nSource: LotusLabs Blog, [Accurate insurance claims prediction with Deep Learning](https://www.lotuslabs.ai/accurate-insurance-claims-prediction-with-deep-learning/).\n:::\n\n## Preprocess all French motor inputs\n\nTransform the categorical variables to integers:\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nnum_brands, num_regions = X_train.nunique()[[\"VehBrand\", \"Region\"]]         #<1>\n\nct = make_column_transformer(\n  (OrdinalEncoder(), [\"VehBrand\", \"Region\", \"Area\", \"VehGas\"]),             #<2>\n  remainder=StandardScaler(),                                               #<3>\n  verbose_feature_names_out=False                                           #<4>\n)\nX_train_ct = ct.fit_transform(X_train)                                      #<5>\nX_test_ct = ct.transform(X_test)                                            #<6>\n```\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n1. Stores separately the number of unique categorical in the nominal variables, as would require these values later for entity embedding\n2. Contructs columns transformer by first ordinally encoding all categorical variables. Ordinal variables are ordinal encoded because it is the sensible thing. Nominal variables are ordinal encoded as an intermediate step before passing through an entity embedding layer \n3. Applies standard scaling to all other numerical variables\n4. `verbose_feature_names_out=False` stops unnecessarily printing out the outputs of the process\n5. Fits the column transformer to the train set and transforms it\n6. Transforms the test set using the column transformer fitted using the train set\n:::\n\nSplit the brand and region data apart from the rest:\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\nX_train_brand = X_train_ct[\"VehBrand\"]; X_test_brand = X_test_ct[\"VehBrand\"]\nX_train_region = X_train_ct[\"Region\"]; X_test_region = X_test_ct[\"Region\"]\nX_train_rest = X_train_ct.drop([\"VehBrand\", \"Region\"], axis=1)\nX_test_rest = X_test_ct.drop([\"VehBrand\", \"Region\"], axis=1)\n```\n:::\n\n\n## Organise the inputs\n\nMake a Keras `Input` for: vehicle brand, region, & others.\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\nveh_brand = Input(shape=(1,), name=\"vehBrand\")\nregion = Input(shape=(1,), name=\"region\")\nother_inputs = Input(shape=X_train_rest.shape[1:], name=\"otherInputs\")\n```\n:::\n\n\nCreate embeddings and join them with the other inputs. \n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\nfrom keras.layers import Reshape                                                #<1>\n\nrandom.seed(1337)\nveh_brand_ee = Embedding(input_dim=num_brands, output_dim=2,                    #<2>\n    name=\"vehBrandEE\")(veh_brand)                                \nveh_brand_ee = Reshape(target_shape=(2,))(veh_brand_ee)                         #<3>\n\nregion_ee = Embedding(input_dim=num_regions, output_dim=2,                      #<4>\n    name=\"regionEE\")(region)\nregion_ee = Reshape(target_shape=(2,))(region_ee)                               #<5>\n\nx = Concatenate(name=\"combined\")([veh_brand_ee, region_ee, other_inputs])       #<6>\n```\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n1. Imports `Reshape` class from `keras.layers` library\n2. Constructs the embedding layer by specifying the input dimension (the number of unique categories) and output dimension (the number of dimensions we want the input to be summarised in to)\n3. Reshapes the output to match the format required at the model building step\n4. Constructs the embedding layer by specifying the input dimension (the number of unique categories) and output dimension\n5. Reshapes the output to match the format required at the model building step\n6. Combines the entity embedded matrices and other inputs together\n:::\n## Complete the model and fit it\n\nFeed the combined embeddings & continuous inputs to some normal dense layers.\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\nx = Dense(30, \"relu\", name=\"hidden\")(x)\nout = Dense(1, \"exponential\", name=\"out\")(x)\n\nmodel = Model([veh_brand, region, other_inputs], out)                     #<1>\nmodel.compile(optimizer=\"adam\", loss=\"poisson\")\n\nhist = model.fit((X_train_brand, X_train_region, X_train_rest),           #<2>\n    y_train, epochs=100, verbose=0,\n    callbacks=[EarlyStopping(patience=5)], validation_split=0.2)\nnp.min(hist.history[\"val_loss\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\n0.6692776679992676\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n1. Model building stage requires all inputs to be passed in together\n2. Passes in the three sets of data, since the format defined at the model building stage requires 3 data sets\n:::\n## Plotting this model\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\nplot_model(model, show_layer_names=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n![](entity-embedding_files/figure-html/cell-51-output-1.png){}\n:::\n:::\n\n\n## Why we need to reshape\n\n::: {.cell execution_count=51}\n``` {.python .cell-code}\nplot_model(model, show_layer_names=True, show_shapes=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n![](entity-embedding_files/figure-html/cell-52-output-1.png){}\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe plotted model shows how, for example, `region` starts off as a matrix with `(None,1)` shape. This indicates that, `region` was a column matrix with some number of rows. Entity embedding the `region` variable resulted in a 3D array of shape (`(None,1,2)`) which is not the required format for concatenating. Therefore, we reshape it using the `Reshape` function. This results in column array of shape, `(None,2)` which is what we need for concatenating.\n:::\n\n# Scale By Exposure {visibility=\"uncounted\"}\n\n## Two different models\n\nHave $\\{ (\\mathbf{x}_i, y_i) \\}_{i=1, \\dots, n}$ for $\\mathbf{x}_i \\in \\mathbb{R}^{47}$ and $y_i \\in \\mathbb{N}_0$.\n\n**Model 1**: Say $Y_i \\sim \\mathsf{Poisson}(\\lambda(\\mathbf{x}_i))$.\n\nBut, the exposures are different for each policy.\n$\\lambda(\\mathbf{x}_i)$ is the expected number of claims for the duration of policy $i$'s contract.\n\n**Model 2**: Say $Y_i \\sim \\mathsf{Poisson}(\\text{Exposure}_i \\times \\lambda(\\mathbf{x}_i))$.\n\nNow, $\\text{Exposure}_i \\not\\in \\mathbf{x}_i$, and $\\lambda(\\mathbf{x}_i)$ is the rate _per year_.\n\n## Just take continuous variables\n\n::: {.content-visible unless-format=\"revealjs\"}\nFor convenience, following code only considers the numerical variables during this implementation.\n:::\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\nct = make_column_transformer(                                           #<1>\n  (\"passthrough\", [\"Exposure\"]),                                        #<2>\n  (\"drop\", [\"VehBrand\", \"Region\", \"Area\", \"VehGas\"]),                   #<3>\n  remainder=StandardScaler(),                                           #<4>\n  verbose_feature_names_out=False                                       #<5>\n)\nX_train_ct = ct.fit_transform(X_train)                                  #<6>\nX_test_ct = ct.transform(X_test)                                        #<7>\n```\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n1. Starts defining the column transformer \n2. Lets `Exposure` passthrough the neural network as it is without peprocessing\n3. Drops the categorical variables (for the ease of implementation)\n4. Scales the remaining variables\n5. Avoids printing unnecessary outputs\n6. Fits and transforms the train set\n7. Only transforms the test set\n:::\n\nSplit exposure apart from the rest:\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\nX_train_exp = X_train_ct[\"Exposure\"]; X_test_exp = X_test_ct[\"Exposure\"]    #<1>\nX_train_rest = X_train_ct.drop(\"Exposure\", axis=1)                          #<2>\nX_test_rest = X_test_ct.drop(\"Exposure\", axis=1)                            #<3>\n```\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n1. Takes out `Exposure` seperately\n2. Drops `Exposure` from train set\n3. Drops `Exposure` from test set\n:::\n\nOrganise the inputs:\n\n::: {.cell execution_count=54}\n``` {.python .cell-code}\nexposure = Input(shape=(1,), name=\"exposure\")\nother_inputs = Input(shape=X_train_rest.shape[1:], name=\"otherInputs\")\n```\n:::\n\n\n## Make & fit the model\n\nFeed the continuous inputs to some normal dense layers.\n\n::: {.cell execution_count=55}\n``` {.python .cell-code}\nrandom.seed(1337)\nx = Dense(30, \"relu\", name=\"hidden1\")(other_inputs)\nx = Dense(30, \"relu\", name=\"hidden2\")(x)\nlambda_ = Dense(1, \"exponential\", name=\"lambda\")(x)\n```\n:::\n\n\n::: {.cell execution_count=56}\n``` {.python .cell-code}\nout = lambda_ * exposure # In past, need keras.layers.Multiply()[lambda_, exposure]\nmodel = Model([exposure, other_inputs], out)\nmodel.compile(optimizer=\"adam\", loss=\"poisson\")\n\nes = EarlyStopping(patience=10, restore_best_weights=True, verbose=1)\nhist = model.fit((X_train_exp, X_train_rest),\n    y_train, epochs=100, verbose=0,\n    callbacks=[es], validation_split=0.2)\nnp.min(hist.history[\"val_loss\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 37: early stopping\nRestoring model weights from the end of the best epoch: 27.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\n0.8827047348022461\n```\n:::\n:::\n\n\n## Plot the model\n\n::: {.cell execution_count=57}\n``` {.python .cell-code}\nplot_model(model, show_layer_names=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n![](entity-embedding_files/figure-html/cell-58-output-1.png){}\n:::\n:::\n\n\n## Package Versions {.appendix data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=58}\n``` {.python .cell-code}\nfrom watermark import watermark\nprint(watermark(python=True, packages=\"keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tf_keras\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.27.0\n\nkeras     : 3.5.0\nmatplotlib: 3.9.2\nnumpy     : 1.26.4\npandas    : 2.2.2\nseaborn   : 0.13.2\nscipy     : 1.11.0\ntorch     : 2.4.1\ntensorflow: 2.17.0\ntf_keras  : 2.17.0\n\n```\n:::\n:::\n\n\n## Glossary {.appendix data-visibility=\"uncounted\"}\n\n::: columns\n:::: column\n- entity embeddings\n- Input layer\n- Keras functional API\n::::\n:::: column\n- Reshape layer\n- skip connection\n- wide & deep network\n::::\n:::\n\n\n\n",
    "supporting": [
      "entity-embedding_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}