{
  "hash": "dc0f0825c468db5d475c19fa58979734",
  "result": {
    "markdown": "---\ntitle: Optimisation\ninclude-in-header:\n  text: <script defer src=\"https://pyscript.net/latest/pyscript.js\"></script>\nresources: \"minimise-with-gradients.py\"\n---\n\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the package imports\"}\nimport random\nimport numpy as np\nimport pandas as pd\n```\n:::\n\n\n:::\n\n\n# Dense Layers in Matrices {data-visibility=\"uncounted\"}\n\n## Logistic regression\n\n\n\n::: columns\n::: column\n\nObservations: $\\mathbf{x}_{i,\\bullet} \\in \\mathbb{R}^{2}$.\n\nTarget: $y_i \\in \\{0, 1\\}$.\n\nPredict: $\\hat{y}_i = \\mathbb{P}(Y_i = 1)$.\n\n<br>\n\n__The model__\n\nFor $\\mathbf{x}_{i,\\bullet} = (x_{i,1}, x_{i,2})$:\n$$\nz_i = x_{i,1} w_1 + x_{i,2} w_2 + b\n$$\n\n$$\n\\hat{y}_i = \\sigma(z_i) = \\frac{1}{1 + \\mathrm{e}^{-z_i}} .\n$$\n\n:::\n::: column\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport sympy\n#sympy.plot(\"1/(1 + exp(-z))\");\n```\n:::\n\n\n:::\n:::\n\n\n\n## Multiple observations\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndata = pd.DataFrame({\"x_1\": [1, 3, 5], \"x_2\": [2, 4, 6], \"y\": [0, 1, 1]})\ndata\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x_1</th>\n      <th>x_2</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet $w_1 = 1$, $w_2 = 2$ and $b = -10$.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nw_1 = 1; w_2 = 2; b = -10\ndata[\"x_1\"] * w_1 + data[\"x_2\"] * w_2 + b \n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n0   -5\n1    1\n2    7\ndtype: int64\n```\n:::\n:::\n\n\n## Matrix notation\n\n::: columns\n::: column\nHave $\\mathbf{X} \\in \\mathbb{R}^{3 \\times 2}$.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nX_df = data[[\"x_1\", \"x_2\"]]\nX = X_df.to_numpy()\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n```\n:::\n:::\n\n\n:::\n::: column\nLet $\\mathbf{w} = (w_1, w_2)^\\top \\in \\mathbb{R}^{2 \\times 1}$.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nw = np.array([[1], [2]])\nw\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([[1],\n       [2]])\n```\n:::\n:::\n\n\n:::\n:::\n\n$$\n\\mathbf{z} = \\mathbf{X} \\mathbf{w} + b , \\quad \\mathbf{a} = \\sigma(\\mathbf{z})\n$$\n\n::: columns\n::: column\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nz = X.dot(w) + b\nz\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([[-5],\n       [ 1],\n       [ 7]])\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n1 / (1 + np.exp(-z))\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\narray([[0.01],\n       [0.73],\n       [1.  ]])\n```\n:::\n:::\n\n\n:::\n:::\n\n## Using a softmax output\n\n::: columns\n::: column\nObservations: $\\mathbf{x}_{i,\\bullet} \\in \\mathbb{R}^{2}$.\nPredict: $\\hat{y}_{i,j} = \\mathbb{P}(Y_i = j)$.\n:::\n::: column\nTarget: $\\mathbf{y}_{i,\\bullet} \\in \\{(1, 0), (0, 1)\\}$.\n:::\n:::\n\n__The model__: For $\\mathbf{x}_{i,\\bullet} = (x_{i,1}, x_{i,2})$\n$$\n\\begin{aligned}\nz_{i,1} &= x_{i,1} w_{1,1} + x_{i,2} w_{2,1} + b_1 , \\\\\nz_{i,2} &= x_{i,1} w_{1,2} + x_{i,2} w_{2,2} + b_2 .\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n\\hat{y}_{i,1} &= \\text{Softmax}_1(\\mathbf{z}_i) = \\frac{\\mathrm{e}^{z_{i,1}}}{\\mathrm{e}^{z_{i,1}} + \\mathrm{e}^{z_{i,2}}} , \\\\\n\\hat{y}_{i,2} &= \\text{Softmax}_2(\\mathbf{z}_i) = \\frac{\\mathrm{e}^{z_{i,2}}}{\\mathrm{e}^{z_{i,1}} + \\mathrm{e}^{z_{i,2}}} .\n\\end{aligned}\n$$\n\n## Multiple observations\n\n::: columns\n::: column\n\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndata\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x_1</th>\n      <th>x_2</th>\n      <th>y_1</th>\n      <th>y_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n::: column\nChoose:\n\n$w_{1,1} = 1$, $w_{2,1} = 2$,\n\n$w_{1,2} = 3$, $w_{2,2} = 4$, and\n\n$b_1 = -10$, $b_2 = -20$.\n\n:::\n:::\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nw_11 = 1; w_21 = 2; b_1 = -10\nw_12 = 3; w_22 = 4; b_2 = -20\ndata[\"x_1\"] * w_11 + data[\"x_2\"] * w_21 + b_1\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n0   -5\n1    1\n2    7\ndtype: int64\n```\n:::\n:::\n\n\n## Matrix notation\n\n::: columns\n::: column\nHave $\\mathbf{X} \\in \\mathbb{R}^{3 \\times 2}$.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n```\n:::\n:::\n\n\n:::\n::: column\n$\\mathbf{W}\\in \\mathbb{R}^{2\\times2}$, $\\mathbf{b}\\in \\mathbb{R}^{2}$\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nW = np.array([[1, 3], [2, 4]])\nb = np.array([-10, -20])\ndisplay(W); b\n```\n\n::: {.cell-output .cell-output-display}\n```\narray([[1, 3],\n       [2, 4]])\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\narray([-10, -20])\n```\n:::\n:::\n\n\n:::\n:::\n\n$$\n  \\mathbf{Z} = \\mathbf{X} \\mathbf{W} + \\mathbf{b} , \\quad \\mathbf{A} = \\text{Softmax}(\\mathbf{Z}) .\n$$\n\n::: columns\n::: column\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nZ = X @ W + b\nZ\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\narray([[-5, -9],\n       [ 1,  5],\n       [ 7, 19]])\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nnp.exp(Z) / np.sum(np.exp(Z),\n  axis=1, keepdims=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray([[9.82e-01, 1.80e-02],\n       [1.80e-02, 9.82e-01],\n       [6.14e-06, 1.00e+00]])\n```\n:::\n:::\n\n\n:::\n:::\n\n\n\n# Optimisation {data-visibility=\"uncounted\"}\n\n## Gradient-based learning\n\n\n\n```{=html}\n<div style=\"font-size: 0px;\">\n  <py-config>packages = [\"matplotlib\"]</py-config>\n  </div>\n<div>\n  <!-- Source for slider with current value shown: https://stackoverflow.com/a/18936328 -->\n  Make a guess: <input type=\"range\" min=\"1\" max=\"100\" value=\"50\" class=\"slider\" id=\"new_guess\" oninput=\"this.nextElementSibling.value = this.value\">\n  <output>50</output><br>\n  Show derivatives: <input type=\"checkbox\" id=\"derivs\">\n  Reveal function: <input type=\"checkbox\" id=\"reveal\">\n</div>\n<div id=\"mpl\" style=\"text-align: center;\"></div>\n<py-script output=\"mpl\" src=\"minimise-with-gradients.py\" />\n```\n\n\n\n## Gradient descent pitfalls\n\n![Potential problems with gradient descent.](Geron-mls2_0406.png)\n\n::: footer\nSource: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 4-6.\n:::\n\n## Go over all the training data\n\n<br>\n\nCalled _batch gradient descent_.\n\n<br>\n\n```python\nfor i in range(num_epochs):\n    gradient = evaluate_gradient(loss_function, data, weights)\n    weights = weights - learning_rate * gradient\n```\n\n## Pick a random training example\n\n<br>\n\nCalled _stochastic gradient descent_.\n\n<br>\n\n```python\nfor i in range(num_epochs):\n    rnd.shuffle(data)\n    for example in data:\n        gradient = evaluate_gradient(loss_function, example, weights)\n        weights = weights - learning_rate * gradient\n```\n\n## Take a group of training examples\n\n<br>\n\nCalled _mini-batch gradient descent_.\n\n<br>\n\n```python\nfor i in range(num_epochs):\n    rnd.shuffle(data)\n    for b in range(num_batches):\n        batch = data[b * batch_size : (b + 1) * batch_size]\n        gradient = evaluate_gradient(loss_function, batch, weights)\n        weights = weights - learning_rate * gradient\n```\n\n## Mini-batch gradient descent\n\n::: columns\n::: column\n\nWhy?\n\n1. Because we have to (data is too big)\n2. Because it is faster (lots of quick noisy steps > a few slow super accurate steps)\n3. The noise helps us jump out of local minima\n\n:::\n::: column\n![Example of jumping from local minima.](Geron-mls2_0406.png)\n:::\n:::\n\n::: footer\nSource: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 4-6.\n:::\n\n## Learning rates\n\n::: columns\n::: column\n\n![The learning rate is too small](Geron-mls2_0404.png)\n:::\n::: column\n![The learning rate is too large](Geron-mls2_0405.png)\n:::\n:::\n\n::: footer\nSource: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figures 4-4 and 4-5.\n:::\n\n## Learning rates #2\n\n![Changing the learning rates for a robot arm.](matt-henderson-learning-rates-animation.mov){width=60%}\n\n::: {.content-visible unless-format=\"revealjs\"}\n\n> \"a nice way to see how the learning rate affects Stochastic Gradient Descent.\n> we can use SGD to control a robot arm - minimizing the distance to the target as a function of the angles θᵢ. Too low a learning rate gives slow inefficient learning, too high and we see instability\"\n\n:::\n\n::: footer\nSource: Matt Henderson (2021), [Twitter post](https://twitter.com/matthen2/status/1520427516997025792)\n:::\n\n## Learning rate schedule\n\n![Learning curves for various learning rates η](Geron-mls2_1108.png)\n\nIn training the learning rate may be tweaked manually.\n\n::: footer\nSource: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 11-8.\n:::\n\n## We need non-zero derivatives {.smaller}\n\nThis is why can't use accuracy as the loss function for classification.\n\nAlso why we can have the _dead ReLU_ problem.\n\n::: {.content-hidden unless-format=\"revealjs\"}\n\n\n{{< video https://www.youtube.com/embed/KpKog-L9veg width=\"100%\" height=\"80%\" >}}\n\n\n\n\n:::\n::: {.content-visible unless-format=\"revealjs\"}\n\n\n{{< video https://www.youtube.com/embed/KpKog-L9veg >}}\n\n\n\n\n:::\n\n# Loss and derivatives {data-visibility=\"uncounted\"}\n\n## Example: linear regression\n\n$$\n\\hat{y}(x) = w x + b\n$$\n\nFor some observation $\\{ x_i, y_i \\}$, the (MSE) loss is\n\n$$ \n\\text{Loss}_i = (\\hat{y}(x_i) - y_i)^2\n$$\n\nFor a batch of the first $n$ observations the loss is\n\n$$ \n\\text{Loss}_{1:n} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}(x_i) - y_i)^2\n$$\n\n## Derivatives\n\nSince $\\hat{y}(x) = w x + b$,\n\n$$\n\\frac{\\partial \\hat{y}(x)}{\\partial w} = x \\text{ and }\n\\frac{\\partial \\hat{y}(x)}{\\partial b} = 1 .\n$$\n\nAs $\\text{Loss}_i = (\\hat{y}(x_i) - y_i)^2$, we know\n$$\n\\frac{\\partial \\text{Loss}_i}{\\partial \\hat{y}(x_i) } = 2 (\\hat{y}(x_i) - y_i) .\n$$\n\n## Chain rule\n\n$$\n\\frac{\\partial \\text{Loss}_i}{\\partial \\hat{y}(x_i) } = 2 (\\hat{y}(x_i) - y_i), \\,\\,\n\\frac{\\partial \\hat{y}(x)}{\\partial w} = x , \\, \\text{ and } \\,\n\\frac{\\partial \\hat{y}(x)}{\\partial b} = 1 .\n$$\n\nPutting this together, we have\n\n$$\n\\frac{\\partial \\text{Loss}_i}{\\partial w}\n= \\frac{\\partial \\text{Loss}_i}{\\partial \\hat{y}(x_i) }\n  \\times \\frac{\\partial \\hat{y}(x_i)}{\\partial w}\n= 2 (\\hat{y}(x_i) - y_i) \\, x_i \n$$\n\nand\n$$\n\\frac{\\partial \\text{Loss}_i}{\\partial b}\n= \\frac{\\partial \\text{Loss}_i}{\\partial \\hat{y}(x_i) }\n  \\times \\frac{\\partial \\hat{y}(x_i)}{\\partial b}\n= 2 (\\hat{y}(x_i) - y_i) .\n$$\n\n## Stochastic gradient descent (SGD)\n\nStart with $\\boldsymbol{\\theta}_0 = (w, b)^\\top = (0, 0)^\\top$.\n\nRandomly pick $i=5$, say $x_i = 5$ and $y_i = 5$.\n\n::: fragment\n$$\n\\hat{y}(x_i) = 0 \\times 5 + 0 = 0 \\Rightarrow \\text{Loss}_i = (0 - 5)^2 = 25.\n$$\n:::\n::: fragment\nThe partial derivatives are\n$$\n\\begin{aligned}\n\\frac{\\partial \\text{Loss}_i}{\\partial w} \n&= 2 (\\hat{y}(x_i) - y_i) \\, x_i = 2 \\cdot (0 - 5) \\cdot 5 = -50, \\text{ and} \\\\\n\\frac{\\partial \\text{Loss}_i}{\\partial b}\n&= 2 (0 - 5) = - 10.\n\\end{aligned}\n$$\nThe gradient is $\\nabla \\text{Loss}_i = (-50, -10)^\\top$.\n:::\n\n## SGD, first iteration\n\nStart with $\\boldsymbol{\\theta}_0 = (w, b)^\\top = (0, 0)^\\top$.\n\nRandomly pick $i=5$, say $x_i = 5$ and $y_i = 5$.\n\nThe gradient is $\\nabla \\text{Loss}_i = (-50, -10)^\\top$.\n\nUse learning rate $\\eta = 0.01$ to update \n$$\n\\begin{aligned}\n\\boldsymbol{\\theta}_1\n&= \\boldsymbol{\\theta}_0 - \\eta \\nabla \\text{Loss}_i \\\\\n&= \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - 0.01 \\begin{pmatrix} -50 \\\\ -10 \\end{pmatrix} \\\\\n&= \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ 0.1 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.1 \\end{pmatrix}.\n\\end{aligned}\n$$\n\n## SGD, second iteration\n\nStart with $\\boldsymbol{\\theta}_1 = (w, b)^\\top = (0.5, 0.1)^\\top$.\n\nRandomly pick $i=9$, say $x_i = 9$ and $y_i = 17$.\n\nThe gradient is $\\nabla \\text{Loss}_i = (-223.2, -24.8)^\\top$.\n\nUse learning rate $\\eta = 0.01$ to update \n$$\n\\begin{aligned}\n\\boldsymbol{\\theta}_2\n&= \\boldsymbol{\\theta}_1 - \\eta \\nabla \\text{Loss}_i \\\\\n&= \\begin{pmatrix} 0.5 \\\\ 0.1 \\end{pmatrix} - 0.01 \\begin{pmatrix} -223.2 \\\\ -24.8 \\end{pmatrix} \\\\\n&= \\begin{pmatrix} 0.5 \\\\ 0.1 \\end{pmatrix} + \\begin{pmatrix} 2.232 \\\\ 0.248 \\end{pmatrix} = \\begin{pmatrix} 2.732 \\\\ 0.348 \\end{pmatrix}.\n\\end{aligned}\n$$\n\n## Batch gradient descent (BGD) {.smaller}\n\nFor the first $n$ observations \n$\\text{Loss}_{1:n} = \\frac{1}{n} \\sum_{i=1}^n \\text{Loss}_i$\nso\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\text{Loss}_{1:n}}{\\partial w}\n&= \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial \\text{Loss}_{i}}{\\partial w}\n= \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial \\text{Loss}_{i}}{\\hat{y}(x_i)} \\frac{\\partial \\hat{y}(x_i)}{\\partial w} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n 2 (\\hat{y}(x_i) - y_i) \\, x_i .\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\text{Loss}_{1:n}}{\\partial b}\n&= \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial \\text{Loss}_{i}}{\\partial b}\n= \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial \\text{Loss}_{i}}{\\hat{y}(x_i)} \\frac{\\partial \\hat{y}(x_i)}{\\partial b} \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n 2 (\\hat{y}(x_i) - y_i) .\n\\end{aligned}\n$$\n\n## BGD, first iteration ($\\boldsymbol{\\theta}_0 = \\boldsymbol{0}$) {.smaller}\n\n\n\n::: {.cell execution_count=20}\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>y_hat</th>\n      <th>loss</th>\n      <th>dL/dw</th>\n      <th>dL/db</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.99</td>\n      <td>0</td>\n      <td>0.98</td>\n      <td>-1.98</td>\n      <td>-1.98</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3.00</td>\n      <td>0</td>\n      <td>9.02</td>\n      <td>-12.02</td>\n      <td>-6.01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>5.01</td>\n      <td>0</td>\n      <td>25.15</td>\n      <td>-30.09</td>\n      <td>-10.03</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nSo $\\nabla \\text{Loss}_{1:3}$ is\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nnabla = np.array([df[\"dL/dw\"].mean(), df[\"dL/db\"].mean()])\nnabla \n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\narray([-14.69,  -6.  ])\n```\n:::\n:::\n\n\nso with $\\eta = 0.1$ then $\\boldsymbol{\\theta}_1$ becomes\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ntheta_1 = theta_0 - 0.1 * nabla\ntheta_1\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\narray([1.47, 0.6 ])\n```\n:::\n:::\n\n\n## BGD, second iteration {.smaller}\n\n\n\n::: {.cell execution_count=24}\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>y_hat</th>\n      <th>loss</th>\n      <th>dL/dw</th>\n      <th>dL/db</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.99</td>\n      <td>2.07</td>\n      <td>1.17</td>\n      <td>2.16</td>\n      <td>2.16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3.00</td>\n      <td>3.54</td>\n      <td>0.29</td>\n      <td>2.14</td>\n      <td>1.07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>5.01</td>\n      <td>5.01</td>\n      <td>0.00</td>\n      <td>-0.04</td>\n      <td>-0.01</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nSo $\\nabla \\text{Loss}_{1:3}$ is\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nnabla = np.array([df[\"dL/dw\"].mean(), df[\"dL/db\"].mean()])\nnabla \n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray([1.42, 1.07])\n```\n:::\n:::\n\n\nso with $\\eta = 0.1$ then $\\boldsymbol{\\theta}_2$ becomes\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ntheta_2 = theta_1 - 0.1 * nabla\ntheta_2\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\narray([1.33, 0.49])\n```\n:::\n:::\n\n\n## Glossary {.appendix data-visibility=\"uncounted\"}\n\n- batches, batch size\n- gradient-based learning, hill-climbing\n- stochastic (mini-batch) gradient descent\n\n",
    "supporting": [
      "optimisation_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}