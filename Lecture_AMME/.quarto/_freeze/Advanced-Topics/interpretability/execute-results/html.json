{
  "hash": "0310f52e2444a84370398caa4e53024e",
  "result": {
    "markdown": "---\ntitle: Interpretability\n---\n\n::: {.content-visible unless-format=\"revealjs\"}\n\n### Acknowledgments\n\nThanks to Eric Dong for making the original version of these slides.\n\n:::\n\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the package imports\"}\nimport random\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.random as rnd\nimport pandas as pd\n\nimport keras\nfrom keras.metrics import SparseTopKCategoricalAccuracy\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input\nfrom keras.callbacks import EarlyStopping\n \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split \n```\n:::\n\n\n:::\n\n# Interpretability {visibility=\"uncounted\"}\n\n::: {.content-visible unless-format=\"revealjs\"}\nInterpretability on a high-level refers to understanding how a model works. Understanding how a model works is very important for decision making. Traditional statistical methods like linear regression and generalized linear regressions are inherently interpretable because we can see and understand how different variables impact the model predictions collectively and individually. In contrast, deep learning algorithms do not readily provide insights into how variables contributed to the predictions. They are composed of multiple layers of interconnected nodes that learn different representations of data. Hence, it is not clear how inputs directly contributed to the outputs. This makes neural networks less interpretable. This is not very desirable, especially in situations which demand making explanations. As such, there is active discussion going on about how we can make less interpretable models more interpretable so that we start trusting these models more.\n:::\n\n<!--\n## Interpretability and Trust\n\nSuppose a neural network informs us to increase the premium for Bob.\n\n-   Why are we getting such a conclusion from the neural network, and should we trust it?\n-   How can we explain our pricing scheme to Bob and the regulators?\n-   Should we be concerned with moral hazards, discrimination, unfairness, and ethical affairs?\n\nWe need to trust the model to employ it! With interpretability, we can trust it!\n-->\n\n## Interpretability\n\nInterpretability Definition\n\n:   *Interpretability refers to the ease with which one can understand and comprehend the model's algorithm and predictions.*\n\nInterpretability of black-box models can be crucial to ascertaining trust.\n\n> Interpretability is about transparency, about understanding exactly why and how the model is generating predictions, and therefore, it is important to observe the inner mechanics of the algorithm considered. This leads to interpreting the model’s parameters and features used to determine the given output. Explainability is about explaining the behavior of the model in human terms.\n\n::: footer\nSource: Charpentier (2024), [_Insurance, Biases, Discrimination and Fairness_](https://link.springer.com/book/10.1007/978-3-031-49783-4), Springer.\n:::\n\n## Husky vs. Wolf\n\n![A well-known anecdote in the explainability literature.](husky.jpg)\n\n::: footer\nRibeiro et al. (2016), _\"Why should I trust you?\" Explaining the predictions of any classifier_, 22nd ACM SIGKDD conference.\n:::\n\n## Aspects of Interpretability\n\nInherent Interpretability\n\n: The model is interpretable by design.\n\n::: {.content-visible unless-format=\"revealjs\"}\nModels with inherent interpretability generally have a simple model architecture where the relationships between inputs and outputs are straightforward. This makes it easy to understand and comprehend model's inner workings and its predictions. As a result, decision making processes convenient. Examples for models with inherent interpretability include linear regression models, generalized linear regression models and decision trees. \n:::\n\nPost-hoc Interpretability\n\n: The model is not interpretable by design, but we can use other methods to explain the model.\n\n::: {.content-visible unless-format=\"revealjs\"}\nPost-hoc interpretability refers to applying various techniques to understand how the model makes its predictions after the model is trained. Post-hoc interpretability is useful for understanding predictions coming from complex models (less interpretable models) such as neural networks, random forests and gradient boosting trees. \n:::\n\n<br>\n\nGlobal Interpretability\n\n: The ability to understand how the model works.\n\nLocal Interpretability\n\n: The ability to interpret/understand each prediction.\n\n::: {.content-visible unless-format=\"revealjs\"}\nGlobal Interpretability focuses on understanding the model's decision-making process as a whole. Global interpretability takes in to account the entire dataset. These techniques will try to look at general patterns related how input data drives the output in general. Examples for techniques include global feature importance method and permutation importance methods.\n\nLocal Interpretability focuses on understanding the model's decision-making for a specific input observation. These techniques will try to look at how different input features contributed to the output. \n:::\n\n# Inherent Interpretability {visibility=\"uncounted\"}\n\n---\n\n![](stop-explaining-black-box.png)\n\n::: footer\nRudin (2019), [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://arxiv.org/pdf/1811.10154.pdf), Nature Machine Intelligence.\n:::\n\n## Trees are interpretable!\n\n![Train prices](opal-train-pricing-tree.png)\n\n## Trees are interpretable?\n\n![Full train pricing](opal-train-pricing-full-tree.png)\n\n## Linear models & LocalGLMNet\n\nA GLM has the form\n\n$$\n\\hat{y} = g^{-1}\\bigl( \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \\bigr)\n$$\n\nwhere $\\beta_0, \\dots, \\beta_p$ are the model parameters.\n\nGlobal & local interpretations are easy to obtain.\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe above GLM representation provides a clear interpretation of how a marginal change in a variable $x$ can contribute to a change in the mean of the output. This makes GLM inherently interpretable.\n:::\n\n<br>\n\n**LocalGLMNet** extends this to a neural network.\n\n$$\n\\hat{y_i} = g^{-1}\\bigl( \\beta_0(\\boldsymbol{x}_i) + \\beta_1(\\boldsymbol{x}_i) x_{i1} + \\dots + \\beta_p(\\boldsymbol{x}_i) x_{ip} \\bigr)\n$$\n\nA GLM with local parameters $\\beta_0(\\boldsymbol{x}_i), \\dots, \\beta_p(\\boldsymbol{x}_i)$ for each observation $\\boldsymbol{x}_i$.\nThe local parameters are the output of a neural network.\n\n::: {.content-visible unless-format=\"revealjs\"}\nHere, $\\beta_p$'s are the neurons from the output layer. First, we define a Feed Foward Neural Network using an input layer, several hidden layers and an output layer.  The number of neurons in the output layer must be equal to the number of inputs. Thereafter, we define a skip connection from the input layer directly to the output layer, and merge them using scaler multiplication. Thereafter, the neural network returns the coefficients of the GLM fitted for each individual. We then train the model with the _response_ variable. \n:::\n\n::: footer\nSource: Richman and Wüthrich (2023), _LocalGLMnet: interpretable deep learning for tabular data_, Scandinavian Actuarial Journal (2023.1), pp. 71-95.\n:::\n\n# Post-hoc Interpretability {visibility=\"uncounted\"}\n\n## Permutation importance {.smaller}\n\n- Inputs: fitted model $m$, tabular dataset $D$.\n- Compute the reference score $s$ of the model $m$ on data\n  $D$ (for instance the accuracy for a classifier or the $R^2$ for\n  a regressor).\n- For each feature $j$ (column of $D$):\n\n  - For each repetition $k$ in ${1, \\dots, K}$:\n\n    - Randomly shuffle column $j$ of dataset $D$ to generate a\n      corrupted version of the data named $\\tilde{D}_{k,j}$.\n    - Compute the score $s_{k,j}$ of model $m$ on corrupted data\n      $\\tilde{D}_{k,j}$.\n\n  - Compute importance $i_j$ for feature $f_j$ defined as:\n\n    $$ i_j = s - \\frac{1}{K} \\sum_{k=1}^{K} s_{k,j} $$\n\nOriginally proposed by Breiman (2001), _Random forests_, Machine learning (45), pp. 5-32.\n\nExtended by Fisher et al. (2019), _All models are wrong, but many are useful: Learning a variable's importance by studying an entire class of prediction models simultaneously_, Journal of Machine Learning Research (20.177), pp. 1-81.\n\n::: footer\nSource: scikit-learn documentation, [permutation_importance function](https://scikit-learn.org/stable/modules/permutation_importance.html).\n:::\n\n## Permutation importance {data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef permutation_test(model, X, y, num_reps=1, seed=42):\n    \"\"\"\n    Run the permutation test for variable importance.\n    Returns matrix of shape (X.shape[1], len(model.evaluate(X, y))).\n    \"\"\"\n    rnd.seed(seed)\n    scores = []    \n\n    for j in range(X.shape[1]):\n        original_column = np.copy(X[:, j])\n        col_scores = []\n\n        for r in range(num_reps):\n            rnd.shuffle(X[:,j])\n            col_scores.append(model.evaluate(X, y, verbose=0))\n\n        scores.append(np.mean(col_scores, axis=0))\n        X[:,j] = original_column\n    \n    return np.array(scores)\n```\n:::\n\n\n## LIME\n\n*Local Interpretable Model-agnostic Explanations* employs an interpretable surrogate model to explain locally how the black-box model makes predictions for individual instances.\n\nE.g. a black-box model predicts Bob's premium as the highest among all policyholders. LIME uses an interpretable model (a linear regression) to explain how Bob's features influence the black-box model's prediction.\n\n::: footer\nSee [\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier](https://youtu.be/hUnRCxnydCc).\n:::\n\n## Globally vs. Locally Faithful\n\nGlobally Faithful\n\n:   *The interpretable model's explanations accurately reflect the behaviour of the black-box model across the entire input space.*\n\nLocally Faithful\n\n:   *The interpretable model's explanations accurately reflect the behaviour of the black-box model for a specific instance.*\n\nLIME aims to construct an interpretable model that mimics the black-box model's behaviour in a *locally faithful* manner.\n\n## LIME Algorithm\n\nSuppose we want to explain the instance $\\boldsymbol{x}_{\\text{Bob}}=(1, 2, 0.5)$.\n\n1.  Generate perturbed examples of $\\boldsymbol{x}_{\\text{Bob}}$ and use the trained gamma MDN $f$ to make predictions:\n    $$\n    \\begin{align*}\n      \\boldsymbol{x}^{'(1)}_{\\text{Bob}} &= (1.1, 1.9, 0.6), \\quad f\\big(\\boldsymbol{x}^{'(1)}_{\\text{Bob}}\\big)=34000 \\\\\n      \\boldsymbol{x}^{'(2)}_{\\text{Bob}} &= (0.8, 2.1, 0.4), \\quad f\\big(\\boldsymbol{x}^{'(2)}_{\\text{Bob}}\\big)=31000 \\\\\n      &\\vdots \\quad \\quad \\quad \\quad\\quad \\quad\\quad \\quad\\quad \\quad \\quad \\vdots\n    \\end{align*}$$\n    We can then construct a dataset of $N_{\\text{Examples}}$ perturbed examples: $\\mathcal{D}_{\\text{LIME}} = \\big(\\big\\{\\boldsymbol{x}^{'(i)}_{\\text{Bob}},f\\big(\\boldsymbol{x}^{'(i)}_{\\text{Bob}}\\big)\\big\\}\\big)_{i=0}^{N_{\\text{Examples}}}$.\n\n## LIME Algorithm\n\n2.  Fit an interpretable model $g$, i.e., a linear regression using $\\mathcal{D}_{\\text{LIME}}$ and the following loss function: $$\\mathcal{L}_{\\text{LIME}}(f,g,\\pi_{\\boldsymbol{x}_{\\text{Bob}}})=\\sum_{i=1}^{N_{\\text{Examples}}}\\pi_{\\boldsymbol{x}_{\\text{Bob}}}\\big(\\boldsymbol{x}^{'(i)}_{\\text{Bob}}\\big)\\cdot \\bigg(f\\big(\\boldsymbol{x}^{'(i)}_{\\text{Bob}}\\big)-g\\big(\\boldsymbol{x}^{'(i)}_{\\text{Bob}}\\big)\\bigg)^2,$$ where $\\pi_{\\boldsymbol{x}_{\\text{Bob}}}\\big(\\boldsymbol{x}^{'(i)}_{\\text{Bob}}\\big)$ represents the distance from the perturbed example $\\boldsymbol{x}^{'(i)}_{\\text{Bob}}$ to the instance to be explained $\\boldsymbol{x}_{\\text{Bob}}$.\n\n## \"Explaining\" to Bob {.smaller}\n\n::: columns\n::: {.column width=\"60%\"}\n![](LIME_Bob.png)\n:::\n::: {.column width=\"40%\"}\nThe bold [red]{style=\"color:red;\"} cross is the instance being explained. LIME samples instances ([grey]{style=\"color:gray;\"} nodes), gets predictions using $f$ (gamma MDN) and weighs them by the proximity to the instance being explained (represented here by size). The dashed line $g$ is the learned local explanation.\n:::\n:::\n\n\n## SHAP Values\n\nThe SHapley Additive exPlanations (SHAP) value helps to quantify the contribution of each feature to the prediction for a specific instance.\n\nThe SHAP value for the $j$th feature is defined as\n$$\\begin{align*}\n\\text{SHAP}^{(j)}(\\boldsymbol{x}) &=\n\\sum_{U\\subset \\{1, ..., p\\} \\backslash \\{j\\}} \\frac{1}{p}\n\\binom{p-1}{|U|}^{-1} \n\\big(\\mathbb{E}[Y| \\boldsymbol{x}^{(U\\cup \\{j\\})}] - \\mathbb{E}[Y|\\boldsymbol{x}^{(U)}]\\big),\n\\end{align*}\n$$\nwhere $p$ is the number of features. A positive SHAP value indicates that the variable increases the prediction value.\n\n::: footer\nReference: Lundberg & Lee (2017), [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/pdf/1705.07874.pdf), Advances in Neural Information Processing Systems, 30.\n:::\n\n<!-- # Explaining Specific Models {visibility=\"uncounted\"} -->\n\n## Grad-CAM\n\n::: columns\n::: column\n![Original image](fountain-square.jpg)\n:::\n::: column\n![Grad-CAM](cam.jpg)\n:::\n:::\n\nSee, e.g., [Keras tutorial](https://keras.io/examples/vision/grad_cam/).\n\n::: footer\nSee Chollet (2021), Deep Learning with Python, Section 9.4.3.\n:::\n\n## Criticism\n\n![Multiple conflicting explanations.](saliency-criticism.jpg)\n\n::: footer\nSource: Rudin (2019), [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://arxiv.org/pdf/1811.10154.pdf), Nature Machine Intelligence.\n:::\n\n\n# Illustrative Example {visibility=\"uncounted\"}\n\n## First attempt at NLP task\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\ndf_raw = pd.read_parquet(\"../Natural-Language-Processing/NHTSA_NMVCCS_extract.parquet.gzip\")\n\ndf_raw[\"NUM_VEHICLES\"] = df_raw[\"NUMTOTV\"].map(lambda x: str(x) if x <= 2 else \"3+\")\n\nweather_cols = [f\"WEATHER{i}\" for i in range(1, 9)]\nfeatures = df_raw[[\"SUMMARY_EN\"] + weather_cols]\n\ntarget_labels = df_raw[\"NUM_VEHICLES\"]\ntarget = LabelEncoder().fit_transform(target_labels)\n\nX_main, X_test, y_main, y_test = train_test_split(features, target, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.25, random_state=1)\n```\n:::\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndf_raw[\"SUMMARY_EN\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0       V1, a 2000 Pontiac Montana minivan, made a lef...\n1       The crash occurred in the eastbound lane of a ...\n2       This crash occurred just after the noon time h...\n                              ...                        \n6946    The crash occurred in the eastbound lanes of a...\n6947    This single-vehicle crash occurred in a rural ...\n6948    This two vehicle daytime collision occurred mi...\nName: SUMMARY_EN, Length: 6949, dtype: object\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndf_raw[\"NUM_VEHICLES\"].value_counts()\\\n  .sort_index()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nNUM_VEHICLES\n1     1822\n2     4151\n3+     976\nName: count, dtype: int64\n```\n:::\n:::\n\n\n:::\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\nTrained neural networks performing really well on predictions does not necessarily imply good performance. Interrogating the model can help us understand inside workings of the model to ensure there are no underlying problems with model.\n:::\n\n::: footer\nSource: [JSchelldorfer's GitHub](https://github.com/JSchelldorfer/ActuarialDataScience/blob/master/12%20-%20NLP%20Using%20Transformers/Actuarial_Applications_of_NLP_Part_1.ipynb).\n:::\n\n## Bag of words for the top 1,000 words\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\ndef vectorise_dataset(X, vect, txt_col=\"SUMMARY_EN\", dataframe=False):\n    X_vects = vect.transform(X[txt_col]).toarray()\n    X_other = X.drop(txt_col, axis=1)\n\n    if not dataframe:\n        return np.concatenate([X_vects, X_other], axis=1)                           \n    else:\n        # Add column names and indices to the combined dataframe.\n        vocab = list(vect.get_feature_names_out())\n        X_vects_df = pd.DataFrame(X_vects, columns=vocab, index=X.index)\n        return pd.concat([X_vects_df, X_other], axis=1) \n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nvect = CountVectorizer(max_features=1_000, stop_words=\"english\")\nvect.fit(X_train[\"SUMMARY_EN\"])\n\nX_train_bow = vectorise_dataset(X_train, vect)\nX_val_bow = vectorise_dataset(X_val, vect)\nX_test_bow = vectorise_dataset(X_test, vect)\n\nvectorise_dataset(X_train, vect, dataframe=True).head()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>10</th>\n      <th>105</th>\n      <th>113</th>\n      <th>12</th>\n      <th>15</th>\n      <th>150</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>180</th>\n      <th>...</th>\n      <th>yield</th>\n      <th>zone</th>\n      <th>WEATHER1</th>\n      <th>WEATHER2</th>\n      <th>WEATHER3</th>\n      <th>WEATHER4</th>\n      <th>WEATHER5</th>\n      <th>WEATHER6</th>\n      <th>WEATHER7</th>\n      <th>WEATHER8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2532</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6209</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2561</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6664</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4214</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1008 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Trained a basic neural network on that\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\ndef build_model(num_features, num_cats):\n    random.seed(42)\n    \n    model = Sequential([\n        Input((num_features,)),\n        Dense(100, activation=\"relu\"),\n        Dense(num_cats, activation=\"softmax\")\n    ])\n    \n    topk = SparseTopKCategoricalAccuracy(k=2, name=\"topk\")\n    model.compile(\"adam\", \"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\", topk])\n    \n    return model\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nnum_features = X_train_bow.shape[1]\nnum_cats = df_raw[\"NUM_VEHICLES\"].nunique()\nmodel = build_model(num_features, num_cats)\nes = EarlyStopping(patience=1, restore_best_weights=True, monitor=\"val_accuracy\")\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nmodel.fit(X_train_bow, y_train, epochs=10,\n    callbacks=[es], validation_data=(X_val_bow, y_val), verbose=0)\nmodel.summary()\n```\n:::\n\n\n::: {.cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,900</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">303</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">303,611</span> (1.16 MB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,203</span> (395.32 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">202,408</span> (790.66 KB)\n</pre>\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nmodel.evaluate(X_train_bow, y_train, verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n[0.021850381046533585, 0.9992803931236267, 1.0]\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nmodel.evaluate(X_val_bow, y_val, verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n[3.338824987411499, 0.9762589931488037, 0.9971222877502441]\n```\n:::\n:::\n\n\n## Permutation importance algorithm {.smaller}\n\nTaken directly from scikit-learn documentation: \n\n- Inputs: fitted predictive model $m$, tabular dataset (training or\n  validation) $D$.\n- Compute the reference score $s$ of the model $m$ on data\n  $D$ (for instance the accuracy for a classifier or the $R^2$ for\n  a regressor).\n- For each feature $j$ (column of $D$):\n\n  - For each repetition $k$ in ${1, \\dots, K}$:\n\n    - Randomly shuffle column $j$ of dataset $D$ to generate a\n      corrupted version of the data named $\\tilde{D}_{k,j}$.\n    - Compute the score $s_{k,j}$ of model $m$ on corrupted data\n      $\\tilde{D}_{k,j}$.\n\n  - Compute importance $i_j$ for feature $f_j$ defined as:\n\n    $$ i_j = s - \\frac{1}{K} \\sum_{k=1}^{K} s_{k,j} $$\n\n::: footer\nSource: scikit-learn documentation, [permutation_importance function](https://scikit-learn.org/stable/modules/permutation_importance.html).\n:::\n\n## Find important inputs\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef permutation_test(model, X, y, num_reps=1, seed=42):\n    \"\"\"\n    Run the permutation test for variable importance.\n    Returns matrix of shape (X.shape[1], len(model.evaluate(X, y))).\n    \"\"\"\n    rnd.seed(seed)\n    scores = []    \n\n    for j in range(X.shape[1]):\n        original_column = np.copy(X[:, j])\n        col_scores = []\n\n        for r in range(num_reps):\n            rnd.shuffle(X[:,j])\n            col_scores.append(model.evaluate(X, y, verbose=0))\n\n        scores.append(np.mean(col_scores, axis=0))\n        X[:,j] = original_column\n    \n    return np.array(scores)\n```\n:::\n\n\n## Run the permutation test\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nall_perm_scores = permutation_test(model, X_val_bow, y_val) #<1>\nall_perm_scores\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\narray([[3.34, 0.98, 1.  ],\n       [3.34, 0.98, 1.  ],\n       [3.34, 0.98, 1.  ],\n       ...,\n       [4.16, 0.98, 1.  ],\n       [4.19, 0.98, 1.  ],\n       [7.87, 0.97, 1.  ]])\n```\n:::\n:::\n\n\n1. The `permutation_test`, aims to evaluate the model's performance on different sets of unseen data. The idea here is to shuffle the order of the _val_ set, and compare the `model` performance.\n\n## Plot the permutated accuracies\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nperm_scores = all_perm_scores[:,1]                                #<1>\nplt.plot(perm_scores)\nplt.xlabel(\"Input index\")\nplt.ylabel(\"Accuracy when shuffled\");\n```\n\n::: {.cell-output .cell-output-display}\n![](interpretability_files/figure-revealjs/cell-18-output-1.png){}\n:::\n:::\n\n\n1. `[:,1]` part will extract the accuracy of the output from the model evaluation and store is as a vector. \n\n::: {.content-visible unless-format=\"revealjs\"}\nThe above method on a high-level says that, if we corrupt the information contained in a feature by changing the order of the data in that feature column, then we are able to see how much information the variable brings in. If a certain variable is not contributing to the prediction accuracy, then changing the order of the variable will not result in a notable drop in accuracy. However, if a certain variable is highly important, then changing the order of data will result in a larger drop. This is an indication of variable importance. The plot above shows how model's accuracy fluctuates across variables, and we can see how certain variables result in larger drops of accuracies.\n:::\n\n## Find the most significant inputs\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nvocab = vect.get_feature_names_out()                            #<1>\ninput_cols = list(vocab) + weather_cols                         #<2>\n\nbest_input_inds = np.argsort(perm_scores)[:100]                 #<3>\nbest_inputs = [input_cols[idx] for idx in best_input_inds]      #<4>\n\nprint(best_inputs)                                              #<5>\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['v3', 'v2', 'vehicle', 'lane', 'harmful', 'right', 'divided', 'south', 'motor', 'dry', 'event', 'left', 'parked', 'WEATHER4', 'related', 'stop', 'impact', 'v4', 'crash', 'direction', 'involved', 'internal', 'stated', 'barrier', 'dodge', 'asphalt', 'chevrolet', 'higher', 'forward', 'pre', 'precrash', 'pushed', 'corner', 'hand', 'prior', 'door', 'WEATHER8', 'factor', 'work', 'mph', 'year', 'critical', 'WEATHER1', 'WEATHER3', 'single', 'WEATHER5', 'straight', 'ahead', 'turning', 'honda', 'hours', 'type', 'daylight', 'possible', 'ford', 'male', 'facing', 'actions', 'consists', 'unknown', 'uphill', 'pick', 'stopped', 'point', 'alcohol', 'high', 'pull', 'proceeded', 'encroaching', 'morning', 'trailer', 'grand', 'associated', 'blood', 'meters', 'basis', 'experience', 'prescription', 'moved', 'small', 'steered', 'maneuver', 'medication', 'heart', 'rotate', 'old', 'pain', 'weekday', 'clear', 'seconds', 'civic', 'started', 'northbound', '2006', 'noon', 'miles', '44', 'injuries', 'vehicles', 'saw']\n```\n:::\n:::\n\n\n1. Extracts the names of the features in a vectorizer object\n2. Combines the list of names in the vectorizer object with the weather columns\n3. Sorts the `perm_scores` in the ascending order and select the 100 observation which had the most impact on model's accuracy\n4. Find the names of the input features by mapping the index\n5. Prints the output\n\n## How about a simple decision tree?\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can try building a simpler model using only the most important features. Here, we chose a classification decision tree.\n:::\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfrom sklearn import tree                                                #<1>\n\nclf = tree.DecisionTreeClassifier(random_state=0, max_leaf_nodes=3)     #<2>\nclf.fit(X_train_bow[:, best_input_inds], y_train);                      #<3>\n```\n:::\n\n\n1. Imports `tree` class from `sklearn`\n2. Specifies a decision tree with 3 leaf nodes. `max_leaf_nodes=3` ensures that the fitted tree will have at most 3 leaf nodes\n3. Fits the decision tree on the selected dataset. Here we only select the `best_input_inds` columns from the train set\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nprint(clf.score(X_train_bow[:, best_input_inds], y_train))\nprint(clf.score(X_val_bow[:, best_input_inds], y_val))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9275605660829935\n0.939568345323741\n```\n:::\n:::\n\n\nThe decision tree ends up giving pretty good results.\n\n## Decision tree \n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ntree.plot_tree(clf, feature_names=best_inputs, filled=True);\n```\n:::\n\n\n::: {.cell execution_count=22}\n\n::: {.cell-output .cell-output-display execution_count=20}\n![](interpretability_files/figure-revealjs/cell-23-output-1.svg){}\n:::\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nprint(np.where(clf.feature_importances_ > 0)[0])\n[best_inputs[ind] for ind in np.where(clf.feature_importances_ > 0)[0]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0 41]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n['v3', 'critical']\n```\n:::\n:::\n\n\n# Illustrative Example (Fixed) {visibility=\"uncounted\"}\n\n## This is why we replace \"v1\", \"v2\", \"v3\"\n\n::: {.cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\n# Go through every summary and find the words \"V1\", \"V2\" and \"V3\".\n# For each summary, replace \"V1\" with a random number like \"V1623\", and \"V2\" with a different random number like \"V1234\".\nrnd.seed(123)\n\ndf = df_raw.copy()\nfor i, summary in enumerate(df[\"SUMMARY_EN\"]):\n    word_numbers = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"]\n    num_cars = 10\n    new_car_nums = [f\"V{rnd.randint(100, 10000)}\" for _ in range(num_cars)]\n    num_spaces = 4\n\n    for car in range(1, num_cars+1):\n        new_num = new_car_nums[car-1]\n        summary = summary.replace(f\"V-{car}\", new_num)\n        summary = summary.replace(f\"Vehicle {word_numbers[car-1]}\", new_num).replace(f\"vehicle {word_numbers[car-1]}\", new_num)\n        summary = summary.replace(f\"Vehicle #{word_numbers[car-1]}\", new_num).replace(f\"vehicle #{word_numbers[car-1]}\", new_num)\n        summary = summary.replace(f\"Vehicle {car}\", new_num).replace(f\"vehicle {car}\", new_num)\n        summary = summary.replace(f\"Vehicle #{car}\", new_num).replace(f\"vehicle #{car}\", new_num)\n        summary = summary.replace(f\"Vehicle # {car}\", new_num).replace(f\"vehicle # {car}\", new_num)\n\n        for j in range(num_spaces+1):\n            summary = summary.replace(f\"V{' '*j}{car}\", new_num).replace(f\"V{' '*j}#{car}\", new_num).replace(f\"V{' '*j}# {car}\", new_num)\n            summary = summary.replace(f\"v{' '*j}{car}\", new_num).replace(f\"v{' '*j}#{car}\", new_num).replace(f\"v{' '*j}# {car}\", new_num)\n         \n    df.loc[i, \"SUMMARY_EN\"] = summary\n```\n:::\n\n\nThere was a slide in the NLP deck titled \"Just ignore this for now...\"\nThat was going through each summary and replacing the words \"V1\", \"V2\", \"V3\" with random numbers. This was done to see if the model was overfitting to these words.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\"}\nfeatures = df[[\"SUMMARY_EN\"] + weather_cols]\nX_main, X_test, y_main, y_test = train_test_split(features, target, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.25, random_state=1)\n\nvect = CountVectorizer(max_features=1_000, stop_words=\"english\")\nvect.fit(X_train[\"SUMMARY_EN\"])\n\nX_train_bow = vectorise_dataset(X_train, vect)\nX_val_bow = vectorise_dataset(X_val, vect)\nX_test_bow = vectorise_dataset(X_test, vect)\n\nmodel = build_model(num_features, num_cats)\n\nes = EarlyStopping(patience=1, restore_best_weights=True,\n    monitor=\"val_accuracy\", verbose=2)\n```\n:::\n\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nmodel.fit(X_train_bow, y_train, epochs=10,\n    callbacks=[es], validation_data=(X_val_bow, y_val), verbose=0);\n```\n:::\n\n\n\n\nRetraining on the fixed dataset gives us a more realistic (lower) accuracy.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nmodel.evaluate(X_train_bow, y_train, verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n[0.1021684780716896, 0.9815303683280945, 0.9990405440330505]\n```\n:::\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nmodel.evaluate(X_val_bow, y_val, verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n[2.4335882663726807, 0.9381294846534729, 0.9942445755004883]\n```\n:::\n:::\n\n\n## Permutation importance accuracy plot\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nperm_scores = permutation_test(model, X_val_bow, y_val)[:,1]\nplt.plot(perm_scores)\nplt.xlabel(\"Input index\"); plt.ylabel(\"Accuracy when shuffled\");\n```\n\n::: {.cell-output .cell-output-display}\n![](interpretability_files/figure-revealjs/cell-31-output-1.png){}\n:::\n:::\n\n\n## Find the most significant inputs\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nvocab = vect.get_feature_names_out()\ninput_cols = list(vocab) + weather_cols\n\nbest_input_inds = np.argsort(perm_scores)[:100]\nbest_inputs = [input_cols[idx] for idx in best_input_inds]\n\nprint(best_inputs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['involved', 'harmful', 'event', 'struck', 'motor', 'higher', 'direction', 'line', 'ford', 'edge', 'single', 'lane', 'contacted', 'left', 'old', 'parked', 'vehicle', 'turned', 'guardrail', 'injured', 'coded', 'legally', 'encroachment', 'critical', 'associated', 'driving', 'intersection', 'location', 'traffic', 'limit', 'rear', 'pushed', 'police', 'stopped', 'include', 'approaching', 'WEATHER5', 'brakes', 'clear', 'WEATHER4', 'way', 'continued', 'counterclockwise', 'turning', 'hit', 'departed', 'driven', 'afternoon', 'northwest', 'stop', 'steered', 'female', 'saw', 'passenger', 'pickup', 'truck', 'actions', 'WEATHER8', '1993', 'properly', 'sky', 'roof', 'sleep', 'road', 'causing', 'approached', 'applied', 'mercury', 'tree', '23', 'possible', 'large', 'check', 'wearing', 'advisory', 'later', 'door', 'divided', 'distraction', '2006', 'treated', 'travels', 'green', 'gmc', 'past', 'path', 'initial', 'final', 'school', 'towed', 'tire', 'time', 'crossing', 'unsuccessful', 'outside', 'route', 'unable', 'adjacent', 'moved', 'southbound']\n```\n:::\n:::\n\n\n## How about a simple decision tree?\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nclf = tree.DecisionTreeClassifier(random_state=0, max_leaf_nodes=3)\nclf.fit(X_train_bow[:, best_input_inds], y_train);\n```\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nprint(clf.score(X_train_bow[:, best_input_inds], y_train))\nprint(clf.score(X_val_bow[:, best_input_inds], y_val))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9179659390741185\n0.9266187050359712\n```\n:::\n:::\n\n\n## Decision tree \n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\ntree.plot_tree(clf, feature_names=best_inputs, filled=True);\n```\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe tree shows how, the model would check for the word _v3_, and decides the prediction as _3+_. This is not very meaningful, because having _v3_ in the input is a direct indication of the number of vehicles. \n:::\n\n::: {.cell execution_count=35}\n\n::: {.cell-output .cell-output-display execution_count=31}\n![](interpretability_files/figure-revealjs/cell-36-output-1.svg){}\n:::\n:::\n\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nprint(np.where(clf.feature_importances_ > 0)[0])\n[best_inputs[ind] for ind in np.where(clf.feature_importances_ > 0)[0]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 1 23]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n['harmful', 'critical']\n```\n:::\n:::\n\n\n## Package Versions {.appendix data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nfrom watermark import watermark\nprint(watermark(python=True, packages=\"keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tf_keras\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.27.0\n\nkeras     : 3.5.0\nmatplotlib: 3.9.2\nnumpy     : 1.26.4\npandas    : 2.2.2\nseaborn   : 0.13.2\nscipy     : 1.11.0\ntorch     : 2.4.1\ntensorflow: 2.17.0\ntf_keras  : 2.17.0\n\n```\n:::\n:::\n\n\n## Glossary {.appendix data-visibility=\"uncounted\"}\n\n- global interpretability\n- Grad-CAM\n- inherent interpretability\n- LIME\n- local interpretability\n- permutation importance\n- post-hoc interpretability\n- SHAP values\n\n",
    "supporting": [
      "interpretability_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}