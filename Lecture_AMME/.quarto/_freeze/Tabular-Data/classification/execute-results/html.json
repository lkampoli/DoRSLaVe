{
  "hash": "a231ecf8dbccd90cb18a02d2449908c0",
  "result": {
    "markdown": "---\ntitle: Classification\n---\n\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the package imports\"}\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n```\n:::\n\n\n:::\n\n# TLDR \n\n## Classification models in Keras\n\n::: {.content-visible unless-format=\"revealjs\"}\nIf the target is categorical variable with only two options, this is a binary classification problem.\nThe neural network's output layer should have **one neuron** with a **sigmoid activation** function.\nThe loss function should be **binary cross-entropy**. In Keras, this is called `loss=\"binary_crossentropy\"`.\n\nIf the target has more than two options, this is a multi-class classification problem.\nThe neural network's output layer should have **as many neurons as there are classes** with a **softmax activation** function.\nThe loss function should be **categorical cross-entropy**. In Keras, this is done with `loss=\"sparse_categorical_crossentropy\"`.\n:::\n\nIf the number of classes is $c$, then:\n\n| Target | Output Layer | Loss Function |\n|--------|--------------|---------------|\n| Binary <br> $(c=2)$ | 1 neuron with `sigmoid` activation | Binary Cross-Entropy |\n| Multi-class <br> $(c > 2)$ | $c$ neurons with `softmax` activation | Categorical Cross-Entropy |\n\n\n## Optionally output logits\n\n::: {.content-visible unless-format=\"revealjs\"}\nIf you find that the training is unstable, you can try to use a **linear activation** in the final layer and the have the loss functions implement the activation function.\n:::\n\nIf the number of classes is $c$, then:\n\n| Target | Output Layer | Loss Function |\n|--------|--------------|---------------|\n| Binary <br> $(c=2)$ | 1 neuron with `linear` activation | Binary Cross-Entropy (`from_logits=True`) |\n| Multi-class <br> $(c > 2)$ | $c$ neurons with `linear` activation | Categorical Cross-Entropy (`from_logits=True`) |\n\n## Code examples {.smaller}\n\n::: columns\n::: column\n**Binary**\n\n```python\nmodel = Sequential([\n  # Skipping the earlier layers\n  Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\")\n```\n:::\n::: column\n**Multi-class**\n\n```python\nmodel = Sequential([\n  # Skipping the earlier layers\n  Dense(n_classes, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\")\n```\n:::\n:::\n\n::: columns\n::: column\n**Binary (logits)**\n\n```python\nfrom keras.losses import BinaryCrossentropy\nmodel = Sequential([\n  # Skipping the earlier layers\n  Dense(1, activation=\"linear\")\n])\nloss = BinaryCrossentropy(from_logits=True)\nmodel.compile(loss=loss)\n```\n:::\n::: column\n**Multi-class (logits)**\n\n```python\nfrom keras.losses import SparseCategoricalCrossentropy\n\nmodel = Sequential([\n  # Skipping the earlier layers\n  Dense(n_classes, activation=\"linear\")\n])\nloss = SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(loss=loss)\n```\n:::\n:::\n\n\n# Classification {visibility=\"uncounted\"}\n\n\n\n## Iris dataset\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_iris\niris = load_iris()\nnames = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]\nfeatures = pd.DataFrame(iris.data, columns=names)\nfeatures\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SepalLength</th>\n      <th>SepalWidth</th>\n      <th>PetalLength</th>\n      <th>PetalWidth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Target variable\n\n::: columns\n::: column\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\niris.target_names\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray(['setosa', 'versicolor', 'virginica'], dtype='<U10')\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\niris.target[:8]\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([0, 0, 0, 0, 0, 0, 0, 0])\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ntarget = iris.target\ntarget = target.reshape(-1, 1)\ntarget[:8]\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([[0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0]])\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nclasses, counts = np.unique(\n        target,\n        return_counts=True\n)\nprint(classes)\nprint(counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0 1 2]\n[50 50 50]\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\niris.target_names[\n  target[[0, 30, 60]]\n]\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([['setosa'],\n       ['setosa'],\n       ['versicolor']], dtype='<U10')\n```\n:::\n:::\n\n\n:::\n:::\n\n\n\n## Split the data into train and test {.smaller}\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nX_train, X_test, y_train, y_test = train_test_split(features, target, random_state=24)\nX_train\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SepalLength</th>\n      <th>SepalWidth</th>\n      <th>PetalLength</th>\n      <th>PetalWidth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>53</th>\n      <td>5.5</td>\n      <td>2.3</td>\n      <td>4.0</td>\n      <td>1.3</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>6.6</td>\n      <td>2.9</td>\n      <td>4.6</td>\n      <td>1.3</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>5.7</td>\n      <td>3.0</td>\n      <td>4.2</td>\n      <td>1.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>6.3</td>\n      <td>2.3</td>\n      <td>4.4</td>\n      <td>1.3</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>7.9</td>\n      <td>3.8</td>\n      <td>6.4</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>112 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nX_test.shape, y_test.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n((38, 4), (38, 1))\n```\n:::\n:::\n\n\n## A basic classifier network\n\n![A basic network for classifying into three categories.](basic-classifier-network.png)\n\n::: footer\nSource: Marcus Lautier (2022).\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\nSince the task is a classification problem, we use `softmax` activation function. The softmax function takes in the input and returns a probability vector, which tells us about the probability of a data point belonging to a certain class.\n:::\n## Create a classifier model\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nNUM_FEATURES = len(features.columns)\nNUM_CATS = len(np.unique(target))\n\nprint(\"Number of features:\", NUM_FEATURES)\nprint(\"Number of categories:\", NUM_CATS)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of features: 4\nNumber of categories: 3\n```\n:::\n:::\n\n\nMake a function to return a Keras model:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndef build_model(seed=42):\n    random.seed(seed)\n    return Sequential([\n        Dense(30, activation=\"relu\"),\n        Dense(NUM_CATS, activation=\"softmax\")\n    ])\n```\n:::\n\n\n## Fit the model\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nmodel = build_model()\nmodel.compile(\"adam\", \"sparse_categorical_crossentropy\")\n\nmodel.fit(X_train, y_train, epochs=5, verbose=2);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/5\n4/4 - 1s - 177ms/step - loss: 1.3502\nEpoch 2/5\n4/4 - 0s - 6ms/step - loss: 1.2852\nEpoch 3/5\n4/4 - 0s - 6ms/step - loss: 1.2337\nEpoch 4/5\n4/4 - 0s - 6ms/step - loss: 1.1915\nEpoch 5/5\n4/4 - 0s - 6ms/step - loss: 1.1556\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nSince the problem at hand is a classification problem, we define the optimizer and loss function accordingly. Optimizer is `adam` and the loss function is `sparse_categorical_crossentropy`. If the response variable represents the category directly using an integer (i.e. if the response variable is not one-hot encoded), we must use `sparse_categorical_crossentropy`. If the response variable (y label) is already one-hot encoded we can use `categorical_crossentropy`. \n:::\n\n## Track accuracy as the model trains\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nmodel = build_model()\nmodel.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.fit(X_train, y_train, epochs=5, verbose=2);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/5\n4/4 - 1s - 200ms/step - accuracy: 0.2946 - loss: 1.3502\nEpoch 2/5\n4/4 - 0s - 6ms/step - accuracy: 0.3036 - loss: 1.2852\nEpoch 3/5\n4/4 - 0s - 6ms/step - accuracy: 0.3036 - loss: 1.2337\nEpoch 4/5\n4/4 - 0s - 6ms/step - accuracy: 0.3304 - loss: 1.1915\nEpoch 5/5\n4/4 - 0s - 6ms/step - accuracy: 0.3393 - loss: 1.1556\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can also specify which loss metric to monitor in assessing the performance during the training. The metric that is usually used in classification tasks is `accuracy`, which tracks the fraction of all predictions which identified the class accurately. The metrics are not used for optimizing. They are only used to keep track of how well the model is performing during the optimization.  By setting `verbose=2`, we are printing the progress during training, and we can see how the loss is reducing and accuracy is improving.\n:::\n\n## Run a long fit\n::: {.content-visible unless-format=\"revealjs\"}\nRun the model training for 500 epochs.\n:::\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nmodel = build_model()\nmodel.compile(\"adam\", \"sparse_categorical_crossentropy\", \\\n        metrics=[\"accuracy\"])\n%time hist = model.fit(X_train, y_train, epochs=500, \\\n        validation_split=0.25, verbose=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 29.3 s, sys: 1.09 s, total: 30.3 s\nWall time: 26.8 s\n```\n:::\n:::\n\n\nEvaluation now returns both _loss_ and _accuracy_.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nmodel.evaluate(X_test, y_test, verbose=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n[0.09586220979690552, 0.9736841917037964]\n```\n:::\n:::\n\n\n## Add early stopping\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nmodel = build_model()                                                   #<1>\nmodel.compile(\"adam\", \"sparse_categorical_crossentropy\", \\\n        metrics=[\"accuracy\"])                                           #<2>\n\nes = EarlyStopping(restore_best_weights=True, patience=50,              #<3>\n        monitor=\"val_accuracy\")                                         \n%time hist_es = model.fit(X_train, y_train, epochs=500, \\\n        validation_split=0.25, callbacks=[es], verbose=False);          #<4>\n\nprint(f\"Stopped after {len(hist_es.history['loss'])} epochs.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 5.79 s, sys: 213 ms, total: 6 s\nWall time: 5.49 s\nStopped after 68 epochs.\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n1. Defines a new model with the same architecture as `model_build` which is already constructed\n2. Compiles the model with optimizer, loss function and metric\n3. Defines the early stopping object as usual, with one slight change. The code is specified to activate the early stopping by monitoring the validation accuracy (`val_accuracy`), not the loss. \n4. Fits the model\n:::\n\nEvaluation on test set:\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nmodel.evaluate(X_test, y_test, verbose=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n[0.9856260418891907, 0.5263158082962036]\n```\n:::\n:::\n\n\n## Fitting metrics\n\n::: columns\n::: column\n\n::: {.cell execution_count=21}\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-revealjs/cell-22-output-1.png){}\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=22}\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-revealjs/cell-23-output-1.png){}\n:::\n:::\n\n\n:::\n:::\n\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nLeft hand side plots show how loss behaved without and with early stopping. Right hand side plots show how accuracy performed without and with early stopping.\n:::\n## What is the softmax activation?\n\nIt creates a \"probability\" vector: $\\text{Softmax}(\\boldsymbol{x}) = \\frac{\\mathrm{e}^x_i}{\\sum_j \\mathrm{e}^x_j} \\,.$\n\nIn NumPy:\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nout = np.array([5, -1, 6])\n(np.exp(out) / np.exp(out).sum()).round(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\narray([0.269, 0.001, 0.731])\n```\n:::\n:::\n\n\nIn Keras:\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nout = keras.ops.convert_to_tensor([[5.0, -1.0, 6.0]])\nkeras.ops.round(keras.ops.softmax(out), 3)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.269, 0.001, 0.731]], dtype=float32)>\n```\n:::\n:::\n\n\n## Prediction using classifiers\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ny_test[:4]\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\narray([[2],\n       [2],\n       [1],\n       [1]])\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe response variable `y` is an array of numeric integers, each representing a class to which the data belongs. However, the `model.predict()` function returns an array with probabilities not an array with integers. The array displays the probabilities of belonging to each category.\n:::\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ny_pred = model.predict(X_test.head(4), verbose=0)\ny_pred\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\narray([[0.13970964, 0.51753014, 0.34276024],\n       [0.24611066, 0.44371164, 0.3101777 ],\n       [0.26309973, 0.43174294, 0.3051573 ],\n       [0.259089  , 0.44883674, 0.29207426]], dtype=float32)\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nUsing `np.argmax()` which returns index of the maximum value in an array, we can obtain the predicted class.\n:::\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n# Add 'keepdims=True' to get a column vector.\nnp.argmax(y_pred, axis=1)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\narray([1, 1, 1, 1])\n```\n:::\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\niris.target_names[np.argmax(y_pred, axis=1)]\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\narray(['versicolor', 'versicolor', 'versicolor', 'versicolor'],\n      dtype='<U10')\n```\n:::\n:::\n\n\n## Cross-entropy loss: ELI5\n\n::: {.content-visible unless-format=\"revealjs\"}\n::: columns\n::: column\n\n\n{{< video https://www.youtube.com/embed/6ArSys5qHAU aspect-ratio=\"1x1\" >}}\n\n\n\n:::\n::: column\n\n\n{{< video https://www.youtube.com/embed/xBEh66V9gZo aspect-ratio=\"1x1\" >}}\n\n\n\n:::\n:::\n:::\n\n::: {.content-hidden unless-format=\"revealjs\"}\n::: columns\n::: column\n\n\n{{< video https://www.youtube.com/embed/6ArSys5qHAU width=\"560\" height=\"315\" >}}\n\n\n\n:::\n::: column\n\n\n{{< video https://www.youtube.com/embed/xBEh66V9gZo width=\"560\" height=\"315\" >}}\n\n\n\n:::\n:::\n:::\n\n\n## Why use cross-entropy loss?\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\np = np.linspace(0, 1, 100)\nplt.plot(p, (1 - p) ** 2)\nplt.plot(p, -np.log(p))\nplt.legend([\"MSE\", \"Cross-entropy\"]);\n```\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-revealjs/cell-31-output-1.png){}\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe above plot shows how MSE and cross-entropy penalize wrong predictions. The x-axis indicates the severity of misclassification. Suppose the neural network predicted that there is near-zero probability of an observation being in class \"1\" when the actual class is \"1\". This represents a strong misclassification. The above graph shows how MSE does not impose heavy penalties for the misclassifications near zero. It displays a linear increment across the severity of misclassification. On the other hand, cross-entropy penalises bad predictions strongly. Also, the misclassification penalty grows exponentially. This makes cross entropy more suitable.\n:::\n\n## One-hot encoding {data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(sparse_output=False)\n\ny_train_oh = enc.fit_transform(y_train)\ny_test_oh = enc.transform(y_test)\n```\n:::\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\ny_train[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\narray([[1],\n       [1],\n       [1],\n       [0],\n       [0]])\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\ny_train_oh[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x0_0</th>\n      <th>x0_1</th>\n      <th>x0_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n:::\n\n## Classifier given one-hot outputs\n\nCreate the model (_new loss function_):\n\n::: {.cell execution_count=34}\n``` {.python .cell-code code-line-numbers=\"|3\"}\nmodel = build_model()\nmodel.compile(\"adam\", \"categorical_crossentropy\", \\\n    metrics=[\"accuracy\"])\n```\n:::\n\n\nFit the model (_new target variables_):\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nmodel.fit(X_train, y_train_oh, epochs=100, verbose=False);\n```\n:::\n\n\nEvaluate the model (_new target variables_):\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nmodel.evaluate(X_test, y_test_oh, verbose=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\n[0.347093790769577, 0.9473684430122375]\n```\n:::\n:::\n\n\n# Stroke Prediction {visibility=\"uncounted\"}\n\n## The data {.smaller}\n\nDataset source: [Kaggle Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset).\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\ndata = pd.read_csv(\"stroke.csv\")\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>hypertension</th>\n      <th>heart_disease</th>\n      <th>ever_married</th>\n      <th>work_type</th>\n      <th>Residence_type</th>\n      <th>avg_glucose_level</th>\n      <th>bmi</th>\n      <th>smoking_status</th>\n      <th>stroke</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9046</td>\n      <td>Male</td>\n      <td>67</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Yes</td>\n      <td>Private</td>\n      <td>Urban</td>\n      <td>228.69</td>\n      <td>36.6</td>\n      <td>formerly smoked</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>51676</td>\n      <td>Female</td>\n      <td>61</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Yes</td>\n      <td>Self-employed</td>\n      <td>Rural</td>\n      <td>202.21</td>\n      <td>NaN</td>\n      <td>never smoked</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31112</td>\n      <td>Male</td>\n      <td>80</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Yes</td>\n      <td>Private</td>\n      <td>Rural</td>\n      <td>105.92</td>\n      <td>32.5</td>\n      <td>never smoked</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>60182</td>\n      <td>Female</td>\n      <td>49</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Yes</td>\n      <td>Private</td>\n      <td>Urban</td>\n      <td>171.23</td>\n      <td>34.4</td>\n      <td>smokes</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1665</td>\n      <td>Female</td>\n      <td>79</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Yes</td>\n      <td>Self-employed</td>\n      <td>Rural</td>\n      <td>174.12</td>\n      <td>24.0</td>\n      <td>never smoked</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Data description {.smaller}\n\n::: columns\n::: column\n1) `id`: unique identifier\n2) `gender`: \"Male\", \"Female\" or \"Other\"\n3) `age`: age of the patient\n4) `hypertension`: 0 or 1 if the patient has hypertension\n5) `heart_disease`: 0 or 1 if the patient has any heart disease\n6) `ever_married`: \"No\" or \"Yes\"\n7) `work_type`: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n:::\n::: column\n8) `Residence_type`: \"Rural\" or \"Urban\"\n9) `avg_glucose_level`: average glucose level in blood\n10) `bmi`: body mass index\n11) `smoking_status`: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"\n12) `stroke`: 0 or 1 if the patient had a stroke\n:::\n:::\n\n::: footer\nSource: Kaggle, [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset).\n:::\n\n## Split the data\n\nFirst, look for missing values.\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nnumber_missing = data.isna().sum()\nnumber_missing[number_missing > 0]\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\nbmi    17\ndtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nfeatures = data.drop([\"id\", \"stroke\"], axis=1)\ntarget = data[\"stroke\"]\n\nX_main, X_test, y_main, y_test = train_test_split(\n    features, target, test_size=0.2, random_state=7)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_main, y_main, test_size=0.25, random_state=12)\n\nX_train.shape, X_val.shape, X_test.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\n((58, 10), (20, 10), (20, 10))\n```\n:::\n:::\n\n\n## What values do we see in the data?\n\n::: columns\n::: column\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\nX_train[\"gender\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\ngender\nFemale    30\nMale      28\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\nX_train[\"ever_married\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\never_married\nYes    51\nNo      7\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\nX_train[\"Residence_type\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\nResidence_type\nUrban    31\nRural    27\nName: count, dtype: int64\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nX_train[\"work_type\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\nwork_type\nPrivate          38\nSelf-employed    16\nGovt_job          4\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\nX_train[\"smoking_status\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\nsmoking_status\nnever smoked       25\nformerly smoked    15\nUnknown            12\nsmokes              6\nName: count, dtype: int64\n```\n:::\n:::\n\n\n:::\n:::\n\n## Preprocess columns individually\n\n1. Take categorical columns $\\hookrightarrow$ one-hot vectors\n2. binary columns $\\hookrightarrow$ do nothing\n3. continuous columns $\\hookrightarrow$ impute NaNs & standardise.\n\n## Scikit-learn column transformer\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nfrom sklearn.pipeline import make_pipeline                                              #<1>\n\ncat_vars =  [\"gender\", \"ever_married\", \"Residence_type\",                                #<2>\n    \"work_type\", \"smoking_status\"]                  \n\nct = make_column_transformer(\n  (OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), cat_vars),              #<3>\n  (\"passthrough\", [\"hypertension\", \"heart_disease\"]),                                   #<4>\n  remainder=make_pipeline(SimpleImputer(), StandardScaler()),                           #<5>\n  verbose_feature_names_out=False\n)\n\nX_train_ct = ct.fit_transform(X_train)\nX_val_ct = ct.transform(X_val)\nX_test_ct = ct.transform(X_test)\n\nfor name, X in zip((\"train\", \"val\", \"test\"), (X_train_ct, X_val_ct, X_test_ct)):        #<6>\n    num_na = X.isna().sum().sum()\n    print(f\"The {name} set has shape {X.shape} & with {num_na} NAs.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe train set has shape (58, 18) & with 0 NAs.\nThe val set has shape (20, 18) & with 0 NAs.\nThe test set has shape (20, 18) & with 0 NAs.\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n1. Imports `make_pipeline` class from `sklearn.pipeline` library. `make_pipeline` is used to streamline the data pre processing. In the above example, `make_pipeline` is used to first treat for missing values and then scale numerical values\n2. Stores categorical variables in `cat_vars`\n3. Specifies the one-hot encoding for all categorical variables. We set the `sparse_output=False`, to return a dense array rather than a sparse matrix. `handle_unknown` specifies how the neural network should handle unseen categories. By setting `handle_unknown=\"ignore\"`, we instruct the neural network to ignore categories that were not seen during training. If we did not do this, it will interrupt the model's operation after deployment\n4. Passes through `hypertension` and `heart_disease` without any pre processing\n5. Makes a pipeline that first applies `SimpleImputer()` to replace missing values with the mean and then applies `StandardScaler()` to scale the numerical values \n6. Prints out the missing values to ensure the `SimpleImputer()` has worked\n:::\n## Handling unseen categories\n\n::: columns\n::: column\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\nX_train[\"gender\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\ngender\nFemale    30\nMale      28\nName: count, dtype: int64\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\nX_val[\"gender\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```\ngender\nMale      13\nFemale     7\nName: count, dtype: int64\n```\n:::\n:::\n\n\n:::\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\nBecause the way train and test was split, one-hot encoder could not pick up on the third category. This could interrupt the model performance. To avoid such confusions, we could either give instructions manually on how to tackle unseen categories. An example is given below.\n:::\n::: columns\n::: column\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\nind = np.argmax(X_val[\"gender\"] == \"Other\")\nX_val.iloc[ind-1:ind+3][[\"gender\"]]\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\ngender_cols = X_val_ct[[\"gender_Female\", \"gender_Male\"]]\ngender_cols.iloc[ind-1:ind+3]\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender_Female</th>\n      <th>gender_Male</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\nHowever, to give such instructions on handling unseen categories, we would first have to know what those possible categories could be. We should also have specific knowledge on what value to assign in case they come up during model performance. One easy way to tackle it would be to use `handle_unknown=\"ignore\"` during encoding, as mentioned before.\n:::\n\n## Setup a binary classification model\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\ndef create_model(seed=42):\n    random.seed(seed)\n    model = Sequential()\n    model.add(Input(X_train_ct.shape[1:]))\n    model.add(Dense(32, \"leaky_relu\"))\n    model.add(Dense(16, \"leaky_relu\"))\n    model.add(Dense(1, \"sigmoid\"))\n    return model\n```\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nSince this is a binary classification problem, we use the sigmoid activation function.\n:::\n\n::: {.cell execution_count=51}\n``` {.python .cell-code}\nmodel = create_model()\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,153</span> (4.50 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,153</span> (4.50 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n`model.summary()` returns the summary of the constructed neural network.\n:::\n\n## Add metrics, compile, and fit\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\nmodel = create_model()                                                  #<1>\n\npr_auc = keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\")                #<2>\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",              #<3>\n    metrics=[pr_auc, \"accuracy\", \"auc\"])                                \n\nes = EarlyStopping(patience=50, restore_best_weights=True,\n    monitor=\"val_pr_auc\", verbose=1)\nmodel.fit(X_train_ct, y_train, callbacks=[es], epochs=1_000, verbose=0,\n  validation_data=(X_val_ct, y_val));\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 51: early stopping\nRestoring model weights from the end of the best epoch: 1.\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n1. Brings in the created model\n2. Creates an instance `pr_auc` to store the AUC (Area Under Curve) metric for the PR (Precision-Recall) curve\n3. Compiles the model with an appropriate loss function, optimizer and relevant metrics. Since the above problem is a binary classification, we would optimize the `binary_crossentropy`, chose to monitor both `accuracy` and `AUC` and `pr_auc`.  \n\nTracking AUC and `pr_auc` on top of the accuracy is important, particularly in the cases where there is a class imbalance. Suppose a data has 95% `True` class and only 5% `False` class, then, even a random classifier that predicts `True` 95% of the time will have a high accuracy. To avoid such issues, it is advisable to monitor both accuracy and AUC.\n:::\n\n::: columns\n::: column\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\nmodel.evaluate(X_val_ct, y_val, verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\n[0.7231547832489014, 1.0, 0.4000000059604645, 0.0]\n```\n:::\n:::\n\n\n:::\n::: column\n:::\n:::\n\n## Overweight the minority class\n\n::: {.cell execution_count=54}\n``` {.python .cell-code}\nmodel = create_model()\n\npr_auc = keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\")\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",\n    metrics=[pr_auc, \"accuracy\", \"auc\"])\n\nes = EarlyStopping(patience=50, restore_best_weights=True,\n    monitor=\"val_pr_auc\", verbose=1)\nmodel.fit(X_train_ct, y_train.to_numpy(), callbacks=[es], epochs=1_000, verbose=0,                    #<1>\n  validation_data=(X_val_ct, y_val), class_weight={0: 1, 1: 10});\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 51: early stopping\nRestoring model weights from the end of the best epoch: 1.\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nAnother way to treat class imbalance would be to assign a higher weight to the minority class during model fitting.\n1. Fits the model by assigning a higher weight to the misclassification in the minor class. This above class weight assignment says that misclassifying an observation from class 1 will be penalized 10 times more than misclassifying an observation from class 0. The weights can be assigned in relation to the level of data imbalance.\n:::\n::: columns\n::: column\n\n::: {.cell execution_count=55}\n``` {.python .cell-code}\nmodel.evaluate(X_val_ct, y_val, verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```\n[0.7231496572494507, 1.0, 0.4000000059604645, 0.0]\n```\n:::\n:::\n\n\n:::\n::: column\n::: fragment\n\n::: {.cell execution_count=56}\n``` {.python .cell-code}\nmodel.evaluate(X_test_ct, y_test, verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\n[0.6236535906791687, 1.0, 0.6499999761581421, 0.0]\n```\n:::\n:::\n\n\n:::\n:::\n:::\n\n## Classification Metrics {.smaller}\n\n::: {.cell execution_count=57}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, RocCurveDisplay, PrecisionRecallDisplay\ny_pred = model.predict(X_test_ct, verbose=0)\n```\n:::\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=58}\n``` {.python .cell-code}\nRocCurveDisplay.from_predictions(y_test, y_pred, name=\"\");\n```\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-revealjs/cell-59-output-1.png){}\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=59}\n``` {.python .cell-code}\nPrecisionRecallDisplay.from_predictions(y_test, y_pred, name=\"\"); plt.legend(loc=\"upper right\");\n```\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-revealjs/cell-60-output-1.png){}\n:::\n:::\n\n\n:::\n:::\n\n::: columns\n::: column\n\n::: {.cell execution_count=60}\n``` {.python .cell-code}\ny_pred_stroke = y_pred > 0.5\nconfusion_matrix(y_test, y_pred_stroke)\n```\n\n::: {.cell-output .cell-output-display execution_count=60}\n```\narray([[ 0,  0],\n       [ 7, 13]])\n```\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=61}\n``` {.python .cell-code}\ny_pred_stroke = y_pred > 0.3\nconfusion_matrix(y_test, y_pred_stroke)\n```\n\n::: {.cell-output .cell-output-display execution_count=61}\n```\narray([[20]])\n```\n:::\n:::\n\n\n:::\n:::\n\n\n\n## Package Versions {.appendix data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=62}\n``` {.python .cell-code}\nfrom watermark import watermark\nprint(watermark(python=True, packages=\"keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tf_keras\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.27.0\n\nkeras     : 3.5.0\nmatplotlib: 3.9.2\nnumpy     : 1.26.4\npandas    : 2.2.2\nseaborn   : 0.13.2\nscipy     : 1.11.0\ntorch     : 2.4.1\ntensorflow: 2.17.0\ntf_keras  : 2.17.0\n\n```\n:::\n:::\n\n\n## Glossary {.appendix data-visibility=\"uncounted\"}\n\n- accuracy\n- classification problem\n- confusion matrix\n- cross-entropy loss\n- metrics\n- sigmoid activation function\n- sofmax activation\n\n",
    "supporting": [
      "classification_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}