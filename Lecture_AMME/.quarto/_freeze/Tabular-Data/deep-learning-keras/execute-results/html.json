{
  "hash": "a2ab4a069ca1bf06fce72a8dfbc31ed5",
  "result": {
    "markdown": "---\ntitle: Deep Learning with Keras\nexecute:\n    warning: false\n---\n\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the package imports\"}\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n```\n:::\n\n\n:::\n\n# California House Price Prediction {visibility=\"uncounted\"}\n\n\n## Data science always starts with the data!\n\n::: columns\n::: column\n\n> The target variable is the median house value for California districts, expressed in $100,000's.\n> This dataset was derived from the 1990 U.S. census, using one row per census block group.\n> A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n\n:::\n::: column\n![Dall-E's rendition of the this dataset.](dalle-california-housing.jpeg)\n:::\n:::\n\n::: footer\nSource: [Scikit-learn documentation](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset).\n:::\n\n## Columns\n\n- `MedInc` median income in block group\n- `HouseAge` median house age in block group\n- `AveRooms` average number of rooms per household\n- `AveBedrms` average # of bedrooms per household\n- `Population` block group population\n- `AveOccup` average number of household members\n- `Latitude` block group latitude\n- `Longitude` block group longitude\n- `MedHouseVal` median house value (**target**)\n\n::: footer\nSource: [Scikit-learn documentation](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset).\n:::\n\n\n## Import the data {.smaller}\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_california_housing                           #<1>\n\nfeatures, target = fetch_california_housing(\n    as_frame=True, return_X_y=True)                                             #<2>\nfeatures                                                                        \n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.3252</td>\n      <td>41.0</td>\n      <td>6.984127</td>\n      <td>1.023810</td>\n      <td>322.0</td>\n      <td>2.555556</td>\n      <td>37.88</td>\n      <td>-122.23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.3014</td>\n      <td>21.0</td>\n      <td>6.238137</td>\n      <td>0.971880</td>\n      <td>2401.0</td>\n      <td>2.109842</td>\n      <td>37.86</td>\n      <td>-122.22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.2574</td>\n      <td>52.0</td>\n      <td>8.288136</td>\n      <td>1.073446</td>\n      <td>496.0</td>\n      <td>2.802260</td>\n      <td>37.85</td>\n      <td>-122.24</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20637</th>\n      <td>1.7000</td>\n      <td>17.0</td>\n      <td>5.205543</td>\n      <td>1.120092</td>\n      <td>1007.0</td>\n      <td>2.325635</td>\n      <td>39.43</td>\n      <td>-121.22</td>\n    </tr>\n    <tr>\n      <th>20638</th>\n      <td>1.8672</td>\n      <td>18.0</td>\n      <td>5.329513</td>\n      <td>1.171920</td>\n      <td>741.0</td>\n      <td>2.123209</td>\n      <td>39.43</td>\n      <td>-121.32</td>\n    </tr>\n    <tr>\n      <th>20639</th>\n      <td>2.3886</td>\n      <td>16.0</td>\n      <td>5.254717</td>\n      <td>1.162264</td>\n      <td>1387.0</td>\n      <td>2.616981</td>\n      <td>39.37</td>\n      <td>-121.24</td>\n    </tr>\n  </tbody>\n</table>\n<p>20640 rows × 8 columns</p>\n</div>\n```\n:::\n:::\n\n\n1. Imports California house prices from `sklearn.datasets` library\n2. Assigns features and target from the dataset to two variables 'features' and 'target' and returns two separate data frames. The command `return_X_y=True` ensures that there will be two separate data frames, one for the features and the other for the target\n\n## What is the target?\n\n::: columns\n::: column\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntarget\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n0        4.526\n1        3.585\n2        3.521\n         ...  \n20637    0.923\n20638    0.847\n20639    0.894\nName: MedHouseVal, Length: 20640, dtype: float64\n```\n:::\n:::\n\n\nWhy predict this? Let's pretend we are these guys.\n:::\n::: column\n![](nytimes-silicon-valley-new-city.png)\n:::\n:::\n\n::: footer\nSource: Dougherty and Griffith (2023), [The Silicon Valley Elite Who Want to Build a City From Scratch](https://www.nytimes.com/2023/08/25/business/land-purchases-solano-county.html), New York Times.\n:::\n\n## An entire ML project\n\n![ML life cycle](ml-lifecycle.png)\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe course focuses more on the modelling part of the life cycle.\n:::\n\n::: footer\nSource: Actuaries Institute, [Do Data Better](https://dodatabetter.com.au/wp-content/uploads/2023/02/Advantage-Graph_1.mp4).\n:::\n\n## Questions to answer in ML project\n\nYou fit a few models to the training set, then ask:\n\n1. __(Selection)__ Which of these models is the best?\n2. __(Future Performance)__ How good should we expect the final model to be on unseen data?\n\n## Set aside a fraction for a test set\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split                        #<1>\n\nX_train, X_test, y_train, y_test = train_test_split(\n    features, target, random_state=42\n)                                                                           #<2>\n```\n:::\n\n\n1. Imports `train_test_split` class from `sklearn.model_selection` library\n2. Splits the dataset into train and the test sets\n\n::: {.content-visible unless-format=\"revealjs\"}\nFirst, we split the data into the train set and the test set using a random selection.  By defining the random state, using the `random_state=42` command, we can ensure that the split is reproducible. We set aside the test data, assuming it represents new, unseen data. Then, we fit many models on the train data and select the one with the lowest train error. Thereafter we assess the performance of that model using the unseen test data.\n:::\n\n::: columns\n::: {.column width=\"70%\"}\n\n![Illustration of a typical training/test split.](heaton-train-test-split.png)\n\n::: {.smaller}\nNote: Compare `X_`/`y_` names, capitals & lowercase.\n:::\n\n:::\n::: {.column width=\"30%\"}\n![Our use of sklearn.](scikit-learn-what-is-my-purpose.png)\n:::\n:::\n\n::: footer\nAdapted from: Heaton (2022), [Applications of Deep Learning](https://github.com/jeffheaton/t81_558_deep_learning/blob/e4bdc124b0c45b592d9bdbed0d2ef6c63c0245d6/t81_558_class_02_1_python_pandas.ipynb), Part 2.1: Introduction to Pandas, and [this random site](https://journeys.dartmouth.edu/folklorearchive/2020/06/03/purpose-of-scikit-learn-is-to-split-the-data/).\n:::\n\n## Basic ML workflow\n\n![Splitting the data.](wiki-ML_dataset_training_validation_test_sets.png)\n\n1. For each model, fit it to the _training set_.\n2. Compute the error for each model on the _validation set_.\n3. Select the model with the lowest validation error.\n4. Compute the error of the final model on the _test set_.\n\n::: footer\nSource: [Wikipedia](https://commons.wikimedia.org/wiki/File:ML_dataset_training_validation_test_sets.png#filelinks).\n:::\n\n## Split three ways\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Thanks https://datascience.stackexchange.com/a/15136\nX_main, X_test, y_main, y_test = train_test_split(\n    features, target, test_size=0.2, random_state=1\n)                                                                   #<1>\n\n# As 0.25 x 0.8 = 0.2\nX_train, X_val, y_train, y_val = train_test_split(\n    X_main, y_main, test_size=0.25, random_state=1\n)                                                                   #<2>\n\nX_train.shape, X_val.shape, X_test.shape                            #<3>\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n((12384, 8), (4128, 8), (4128, 8))\n```\n:::\n:::\n\n\n1. Splits the entire dataset into two parts. Sets aside $20\\%$ of the data as the test set.\n2. Splits the first $80\\%$ of the data (`X_main` and `y_main`)  further into train and validation sets. Sets aside $25\\%$ as the validation set\n\n::: {.content-visible unless-format=\"revealjs\"}\nThis results in $60:20:20$ three way split. While this is not a strict rule, it is widely used. \n:::\n\n## Why not use test set for both?\n\n_Thought experiment_: have $m$ classifiers: $f_1(\\mathbf{x})$, $\\dots$, $f_m(\\mathbf{x})$.\n\nThey are just as good as each other in the long run\n$$\n\\mathbb{P}(\\, f_i(\\mathbf{X}) = Y \\,)\\ =\\ 90\\% , \\quad \\text{for } i=1,\\dots,m .\n$$\n\n::: columns\n::: {.column width=\"40%\"}\nEvaluate each model on the test set, some will be better than others.\n\n:::\n::: {.column width=\"60%\"}\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-8-output-1.png){}\n:::\n:::\n\n\n:::\n:::\n\nTake the best, you'd think it has $\\approx 98\\%$ accuracy!\n\n::: {.content-visible unless-format=\"revealjs\"}\nUsing the same dataset for both validating and testing purposes can result in a data leakage. The information from supposedly 'unseen' data is now used by the model during its tuning. This results in a situation where the model is now 'learning' from the test data, and it could lead to overly optimistic results in the model evaluation stage.\n:::\n\n# EDA & Baseline Model {data-visibility=\"uncounted\"}\n\n## The training set {.smaller}\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nX_train\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9107</th>\n      <td>4.1573</td>\n      <td>19.0</td>\n      <td>6.162630</td>\n      <td>1.048443</td>\n      <td>1677.0</td>\n      <td>2.901384</td>\n      <td>34.63</td>\n      <td>-118.18</td>\n    </tr>\n    <tr>\n      <th>13999</th>\n      <td>0.4999</td>\n      <td>10.0</td>\n      <td>6.740000</td>\n      <td>2.040000</td>\n      <td>108.0</td>\n      <td>2.160000</td>\n      <td>34.69</td>\n      <td>-116.90</td>\n    </tr>\n    <tr>\n      <th>5610</th>\n      <td>2.0458</td>\n      <td>27.0</td>\n      <td>3.619048</td>\n      <td>1.062771</td>\n      <td>1723.0</td>\n      <td>3.729437</td>\n      <td>33.78</td>\n      <td>-118.26</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8539</th>\n      <td>4.0727</td>\n      <td>18.0</td>\n      <td>3.957845</td>\n      <td>1.079625</td>\n      <td>2276.0</td>\n      <td>2.665105</td>\n      <td>33.90</td>\n      <td>-118.36</td>\n    </tr>\n    <tr>\n      <th>2155</th>\n      <td>2.3190</td>\n      <td>41.0</td>\n      <td>5.366265</td>\n      <td>1.113253</td>\n      <td>1129.0</td>\n      <td>2.720482</td>\n      <td>36.78</td>\n      <td>-119.79</td>\n    </tr>\n    <tr>\n      <th>13351</th>\n      <td>5.5632</td>\n      <td>9.0</td>\n      <td>7.241087</td>\n      <td>0.996604</td>\n      <td>2280.0</td>\n      <td>3.870968</td>\n      <td>34.02</td>\n      <td>-117.62</td>\n    </tr>\n  </tbody>\n</table>\n<p>12384 rows × 8 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Location\n\nPython's `matplotlib` package $\\approx$ R's basic `plot`s.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.scatter(features[\"Longitude\"], features[\"Latitude\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n<matplotlib.collections.PathCollection at 0x7f6cf67313d0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-10-output-2.png){}\n:::\n:::\n\n\n::: {.callout-note .fragment}\nThere's no _analysis_ in this EDA.\n:::\n\n## Location EDA\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nplt.scatter(features[\"Longitude\"], features[\"Latitude\"], c=target, cmap=\"coolwarm\")\nplt.colorbar()\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n<matplotlib.colorbar.Colorbar at 0x7f6cf20c1310>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-11-output-2.png){}\n:::\n:::\n\n\n::: {.fragment}\n\"We observe that the median house prices are higher closer to the coastline.\"\n:::\n\n## Pandas can make plots directly\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nboth = pd.concat([features, target], axis=1)\nboth.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", c=\"MedHouseVal\", cmap=\"coolwarm\")\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n<Axes: xlabel='Longitude', ylabel='Latitude'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-12-output-2.png){}\n:::\n:::\n\n\n## Features\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nprint(list(features.columns))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n```\n:::\n:::\n\n\nHow many?\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nnum_features = len(features.columns)\nnum_features\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n8\n```\n:::\n:::\n\n\nOr\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nnum_features = features.shape[1]\nfeatures.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n(20640, 8)\n```\n:::\n:::\n\n\n## Linear Regression\n\n$$ \\hat{y}_i = w_0 + \\sum_{j=1}^p w_j x_{ij} .$$\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression                           #<1>\n\nlr = LinearRegression()                                                     #<2>\nlr.fit(X_train, y_train);                                                   #<3>\n```\n:::\n\n\n1. Imports the `LinearRegression` class from the `sklearn.linear_model` module\n2. Defines the object `lr` which represents the linear regression function\n3. Fits a linear regression model using train data. `lr.fit` computes the coefficients of the regression model\n\n\nThe $w_0$ is in `lr.intercept_` and the others are in\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nprint(lr.coef_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 4.34267965e-01  9.88284781e-03 -9.39592954e-02  5.86373944e-01\n -1.58360948e-06 -3.59968968e-03 -4.26013498e-01 -4.41779336e-01]\n```\n:::\n:::\n\n\n## Make some predictions\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nX_train.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9107</th>\n      <td>4.1573</td>\n      <td>19.0</td>\n      <td>6.162630</td>\n      <td>1.048443</td>\n      <td>1677.0</td>\n      <td>2.901384</td>\n      <td>34.63</td>\n      <td>-118.18</td>\n    </tr>\n    <tr>\n      <th>13999</th>\n      <td>0.4999</td>\n      <td>10.0</td>\n      <td>6.740000</td>\n      <td>2.040000</td>\n      <td>108.0</td>\n      <td>2.160000</td>\n      <td>34.69</td>\n      <td>-116.90</td>\n    </tr>\n    <tr>\n      <th>5610</th>\n      <td>2.0458</td>\n      <td>27.0</td>\n      <td>3.619048</td>\n      <td>1.062771</td>\n      <td>1723.0</td>\n      <td>3.729437</td>\n      <td>33.78</td>\n      <td>-118.26</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n`X_train.head(3)` returns the first three rows of the dataset `X_train`.   \n:::\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ny_pred = lr.predict(X_train.head(3))\ny_pred\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray([1.81699287, 0.0810446 , 1.62089363])\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n`lr.predict(X_train.head(3))` returns the predictions for the first three rows of the dataset `X_train`.\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can manually calculate predictions using the linear regression model to verify the output of the `lr.predict()` function. In the following code, we first define $w_0$ as the intercept of the `lr` function (initial value for the prediction calculation), and then keep on adding the $w_j \\times x_j$ terms\n:::\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nprediction = lr.intercept_                               #<1>\nfor w_j, x_0j in zip(lr.coef_, X_train.iloc[0]):         #<2>\n    prediction += w_j * x_0j                             #<3>\nprediction                                              \n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n1.8169928680677785\n```\n:::\n:::\n\n\n1. Specifies the value of the intercept from the fitted regression as `prediction`\n2. Iterates over the first observation from the train data (`X_train`) and the corresponding weight coefficients from the fitted linear regression\n3. Updates the prediction value\n\n\n## Plot the predictions\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=21}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-22-output-1.png){}\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=22}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-23-output-1.png){}\n:::\n:::\n\n\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can see how both plots have a dispersion to either sides of the fitted line.\n:::\n\n## Calculate mean squared error\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nimport pandas as pd\n\ny_pred = lr.predict(X_train)\ndf = pd.DataFrame({\"Predictions\": y_pred, \"True values\": y_train})\ndf[\"Squared Error\"] = (df[\"Predictions\"] - df[\"True values\"]) ** 2\ndf.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Predictions</th>\n      <th>True values</th>\n      <th>Squared Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9107</th>\n      <td>1.816993</td>\n      <td>2.281</td>\n      <td>0.215303</td>\n    </tr>\n    <tr>\n      <th>13999</th>\n      <td>0.081045</td>\n      <td>0.550</td>\n      <td>0.219919</td>\n    </tr>\n    <tr>\n      <th>5610</th>\n      <td>1.620894</td>\n      <td>1.745</td>\n      <td>0.015402</td>\n    </tr>\n    <tr>\n      <th>13533</th>\n      <td>1.168949</td>\n      <td>1.199</td>\n      <td>0.000903</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ndf[\"Squared Error\"].mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n0.5291948207479792\n```\n:::\n:::\n\n\n## Using `mean_squared_error`\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ndf[\"Squared Error\"].mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n0.5291948207479792\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can compute the mean squared error to evaluate, on average, the accuracy of the predictions. To do this, we first create a data frame using pandas `DataFrame` function. It will have two columns, one with the predicted values and the other with the actual values. Next, we add another column to the same data frame using `df[\"Squared Error\"]` that computes and stores the squared error for each row. Using the function `df[\"Squared Error\"].mean()`, we extract the column 'Squared Error' from the data frame 'df' and calculate the 'mean'.\n:::\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error as mse\n\nmse(y_train, y_pred)\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n0.5291948207479792\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can also use the function `mean_squared_error` from `sklearn.metrics` library to calculate the same. \n:::\n\nStore the results in a dictionary:\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nmse_lr_train = mse(y_train, lr.predict(X_train))\nmse_lr_val = mse(y_val, lr.predict(X_val))\n\nmse_train = {\"Linear Regression\": mse_lr_train}\nmse_val = {\"Linear Regression\": mse_lr_val}\n```\n:::\n\n\n::: {.callout-tip}\nThink about the units of the mean squared error.\nIs there a variation which is more interpretable?\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\nStoring results in data structures like dictionaries is a good practice that can help in managing and handling data efficiently. \n:::\n\n# Our First Neural Network {visibility=\"uncounted\"}\n\n## What are Keras and TensorFlow?\n\nKeras is common way of specifying, training, and using neural networks.\nIt gives a simple interface to _various backend_ libraries, including Tensorflow. \n\n![Keras as a independent interface, and Keras as part of Tensorflow.](Geron-mls2_1010.png)\n\n:::footer\nSource: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 10-10.\n:::\n\n## Create a Keras ANN model\n\nDecide on the architecture: a simple fully-connected network with one hidden layer with 30 neurons.\n\nCreate the model:\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nfrom keras.models import Sequential                              #<1>\nfrom keras.layers import Dense, Input                            #<2>\n\nmodel = Sequential(\n    [Input((num_features,)),\n     Dense(30, activation=\"leaky_relu\"),\n     Dense(1, activation=\"leaky_relu\")]\n)                                                                #<3>\n```\n:::\n\n\n1. Imports `Sequential` from `keras.models`\n2. Imports `Dense` from `keras.layers`\n3. Defines the model architecture using `Sequential()` function\n\n::: {.content-visible unless-format=\"revealjs\"}\nThis neural network architecture includes one hidden layer with 30 neurons and an output layer with 1 neuron. While there is an activation function specified (`leaky_relu`) for the hidden layer, there is no activation function specified for the output layer. In situations where there is no specification, the output layer assumes a `linear` activation.\n:::\n\n## Inspect the model\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">270</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">301</span> (1.18 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">301</span> (1.18 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n```\n:::\n:::\n\n\n## The model is initialised randomly\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nmodel = Sequential([Dense(30, activation=\"leaky_relu\"), Dense(1, activation=\"leaky_relu\")])\nmodel.predict(X_val.head(3), verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\narray([[-91.88699  ],\n       [-57.336792 ],\n       [ -1.2164341]], dtype=float32)\n```\n:::\n:::\n\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nmodel = Sequential([Dense(30, activation=\"leaky_relu\"), Dense(1, activation=\"leaky_relu\")])\nmodel.predict(X_val.head(3), verbose=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\narray([[-63.595776],\n       [-34.140812],\n       [ 17.690414]], dtype=float32)\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can see how rerunning the same code with the same input data results in significantly different predictions. This is due to the random initialization. \n:::\n## Controlling the randomness\n\n::: {.cell execution_count=33}\n``` {.python .cell-code code-line-numbers=\"|1-3,9\"}\nimport random\n\nrandom.seed(123)\n\nmodel = Sequential([Dense(30, activation=\"leaky_relu\"), Dense(1, activation=\"leaky_relu\")])\n\ndisplay(model.predict(X_val.head(3), verbose=0))\n\nrandom.seed(123)\nmodel = Sequential([Dense(30, activation=\"leaky_relu\"), Dense(1, activation=\"leaky_relu\")])\n\ndisplay(model.predict(X_val.head(3), verbose=0))\n```\n\n::: {.cell-output .cell-output-display}\n```\narray([[ 1.3595750e+03],\n       [ 8.2818066e+02],\n       [-1.2993935e+00]], dtype=float32)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\narray([[ 1.3595750e+03],\n       [ 8.2818066e+02],\n       [-1.2993935e+00]], dtype=float32)\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nBy setting the seed, we can control for the randomness.\n:::\n\n## Fit the model\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nrandom.seed(123)\n\nmodel = Sequential([\n    Dense(30, activation=\"leaky_relu\"),\n    Dense(1, activation=\"leaky_relu\")\n])\n\nmodel.compile(\"adam\", \"mse\")\n%time hist = model.fit(X_train, y_train, epochs=5, verbose=False)\nhist.history[\"loss\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 3.14 s, sys: 266 ms, total: 3.41 s\nWall time: 2.23 s\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\n[18765.189453125,\n 178.2383270263672,\n 103.30644226074219,\n 48.04048538208008,\n 18.11100959777832]\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe above code explains how we would fit a basic neural network. First, we define the seed for reproducibility. Next, we define the architecture of the model. Thereafter, we compile the model. Compiling involves giving instructions on how we want the model to be trained. At the least, we must define the optimizer and loss function. The optimizer explains how the model should learn (how the model should update the weights), and the loss function states the objective that the model needs to optimize. In the above code, we use `adam` as the optimizer and `mse` (mean squared error) as the loss function. After compilation, we fit the model. The `fit()` function takes in the training data, and runs the entire dataset through 5 epochs before training completes. What this means is that the model is run through the entire dataset 5 times. Suppose we start the training process with the random initialization, run the model through the entire data, calculate the `mse` (after 1 epoch), and update the weights using the `adam` optimizer. Then we run the model through the entire dataset once again with the updated weights, to calculate the `mse` at the end of the second epoch. Likewise, we would run the model 5 times before the training completes. `hist.history()` function returns the calculate `mse` at each step.\n:::\n\n<br>\n\n::: {.content-visible unless-format=\"revealjs\"}\n`%time` command computes and prints the amount of time spend on training.\nBy setting `verbose=False` we can avoid printing of intermediate results during training. Setting `verbose=True` is useful when we want to observe how the neural network is training.  \n:::\n\n## Make predictions\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\ny_pred = model.predict(X_train[:3], verbose=0)\ny_pred\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\narray([[ 0.5477159 ],\n       [-1.5254494 ],\n       [-0.25847745]], dtype=float32)\n```\n:::\n:::\n\n\n::: {.callout-note}\n\nThe `.predict` gives us a 'matrix' not a 'vector'.\nCalling `.flatten()` will convert it to a 'vector'.\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nprint(f\"Original shape: {y_pred.shape}\")\ny_pred = y_pred.flatten()\nprint(f\"Flattened shape: {y_pred.shape}\")\ny_pred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal shape: (3, 1)\nFlattened shape: (3,)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\narray([ 0.5477159 , -1.5254494 , -0.25847745], dtype=float32)\n```\n:::\n:::\n\n\n:::\n\n## Plot the predictions\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=38}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-39-output-1.png){}\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=39}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-40-output-1.png){}\n:::\n:::\n\n\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nOne problem with the predictions is that lots of predictions include negative values, which is unrealistic for house prices. We might have to rethink the activation function in the output layer.\n:::\n\n## Assess the model\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\ny_pred = model.predict(X_val, verbose=0)\nmse(y_val, y_pred)\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\n8.391647186561524\n```\n:::\n:::\n\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\nmse_train[\"Basic ANN\"] = mse(\n    y_train, model.predict(X_train, verbose=0)\n)\nmse_val[\"Basic ANN\"] = mse(y_val, model.predict(X_val, verbose=0))\n```\n:::\n\n\nSome predictions are negative:\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\ny_pred = model.predict(X_val, verbose=0)\ny_pred.min(), y_pred.max()\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\n(-5.3710294, 16.863848)\n```\n:::\n:::\n\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\ny_val.min(), y_val.max()\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\n(0.225, 5.00001)\n```\n:::\n:::\n\n\n# Force positive predictions {visibility=\"uncounted\"}\n\n## Try running for longer\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nrandom.seed(123)\n\nmodel = Sequential([\n    Dense(30, activation=\"leaky_relu\"),\n    Dense(1, activation=\"leaky_relu\")\n])\n\nmodel.compile(\"adam\", \"mse\")\n\n%time hist = model.fit(X_train, y_train, epochs=50, verbose=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 26 s, sys: 2.03 s, total: 28 s\nWall time: 16.1 s\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe will train the same neural network architecture with more epochs (`epochs=50`) to see if the results improve.\n:::\n## Loss curve\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\nplt.plot(range(1, 51), hist.history[\"loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE\");\n```\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-47-output-1.png){}\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe loss curve experiences a sudden drop even before finishing 5 epochs and remains consistently low. This indicates that increasing the number of epochs from 5 to 50 does not significantly increase the accuracy.\n:::\n\n## Loss curve {data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\nplt.plot(range(2, 51), hist.history[\"loss\"][1:])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE\");\n```\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-48-output-1.png){}\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nThe above code filters out the MSE value from the first epoch. It plots the vector of MSE values starting from the 2nd epoch. By doing so, we can observe the fluctuations in the MSE values across different epochs more clearly. Results show that the model does not benefit from increasing the epochs.\n:::\n\n## Predictions\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\ny_pred = model.predict(X_val, verbose=0)\nprint(f\"Min prediction: {y_pred.min():.2f}\")\nprint(f\"Max prediction: {y_pred.max():.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMin prediction: -4.07\nMax prediction: 7.78\n```\n:::\n:::\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\nplt.scatter(y_pred, y_val)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"True values\")\nadd_diagonal_line()\n```\n:::\n\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\nmse_train[\"Long run ANN\"] = mse(\n    y_train, model.predict(X_train, verbose=0)\n)\nmse_val[\"Long run ANN\"] = mse(y_val, model.predict(X_val, verbose=0))\n```\n:::\n\n\n:::\n::: column\n\n<div style=\"position: relative; top: -2em;\">\n\n::: {.cell execution_count=51}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-52-output-1.png){}\n:::\n:::\n\n\n</div>\n\n:::\n:::\n\n## Try different activation functions\n\n::: {.cell execution_count=52}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-53-output-1.png){}\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe should be mindful when selecting the activation function. Both `tanh` and `sigmoid` functions restrict the output values to the range of [0,1]. This is not sensible for house price modelling. `softplus` does not have that problem. Also, `softplus` ensures the output is positive which is realistic for house prices.\n:::\n## Enforce positive outputs (softplus)\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\nrandom.seed(123)\n\nmodel = Sequential([\n    Dense(30, activation=\"leaky_relu\"),\n    Dense(1, activation=\"softplus\")\n])\n\nmodel.compile(\"adam\", \"mse\")\n\n%time hist = model.fit(X_train, y_train, epochs=50, \\\n    verbose=False)\n\nimport numpy as np\nlosses = np.round(hist.history[\"loss\"], 2)\nprint(losses[:5], \"...\", losses[-5:])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 26.8 s, sys: 1.58 s, total: 28.3 s\nWall time: 16.4 s\n[1.856457e+04 5.640000e+00 5.640000e+00 5.640000e+00 5.640000e+00] ... [5.64 5.64 5.64 5.64 5.64]\n```\n:::\n:::\n\n\n## Plot the predictions\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=55}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-56-output-1.png){}\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=56}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-57-output-1.png){}\n:::\n:::\n\n\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nPlots illustrate how all the outputs were stuck at zero. Irrespective of how many epochs we run, the output would always be zero.\n:::\n\n## Enforce positive outputs ($\\mathrm{e}^{\\,x}$)\n\n::: {.cell execution_count=58}\n``` {.python .cell-code code-line-numbers=\"|5\"}\nrandom.seed(123)\n\nmodel = Sequential([\n    Dense(30, activation=\"leaky_relu\"),\n    Dense(1, activation=\"exponential\")\n])\n\nmodel.compile(\"adam\", \"mse\")\n\n%time hist = model.fit(X_train, y_train, epochs=5, verbose=False)\n\nlosses = hist.history[\"loss\"]\nprint(losses)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 3.42 s, sys: 170 ms, total: 3.59 s\nWall time: 2.35 s\n[nan, nan, nan, nan, nan]\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nTraining the model again with an exponential activation function will give `nan` values. This is because the results then can explode easily.\n:::\n\n## Same as transforming the target\n\n![The polynomial regression used by researchers who first studied this dataset.](california-housing-linear-model.png)\n\n::: {.callout-note}\nFitting $\\ln(\\text{Median Value})$ is mathematically identical to the `exponential` activation function in the final layer (but metrics are in different units).\n:::\n\n::: footer\nSource: Pace and Barry (1997), [Sparse Spatial Autoregressions](http://www.sciencedirect.com/science/article/pii/S0167-7152(96)00140-X), Statistics & Probability Letters.\n:::\n\n## Good to know others results\n\n![That basic model gets $R^2$ of 0.61, but their fancy model gets 0.86.](california-housing-linear-model-results.png)\n\n::: footer\nSource: Pace and Barry (1997), [Sparse Spatial Autoregressions](http://www.sciencedirect.com/science/article/pii/S0167-7152(96)00140-X), Statistics & Probability Letters.\n:::\n\n## GPT can double-check these results\n\n::: columns\n::: column\n![Asking GPT to check it.](gpt-validating-cali-housing-regression.png)\n\nI'd previously given it the CSV of the data.\n:::\n::: column\n![The code it wrote & ran.](gpt-cali-housing-code.png)\n:::\n:::\n\n# Preprocessing {visibility=\"uncounted\"}\n\n## Re-scaling the inputs\n\n::: {.cell execution_count=59}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_sc = scaler.transform(X_train)\nX_val_sc = scaler.transform(X_val)\nX_test_sc = scaler.transform(X_test)\n```\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nNote: We apply both the `fit` and `transform` operations on the train data. However, we only apply `transform` on the validation and test data.\n:::\n\n::: columns\n::: column\n\n::: {.cell output-location='default' execution_count=60}\n``` {.python .cell-code}\nplt.hist(X_train.iloc[:, 0])\nplt.hist(X_train_sc[:, 0])\nplt.legend([\"Original\", \"Scaled\"]);\n```\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=61}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-62-output-1.png){}\n:::\n:::\n\n\n:::\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\nWe can see how the original values for the input varied between 0 and 10, and how the scaled input values are now between -2 and 2.5. Neural networks prefer if the inputs range between -1 and 1. \n:::\n## Same model with scaled inputs\n\n::: {.cell execution_count=62}\n``` {.python .cell-code code-line-numbers=\"|11\"}\nrandom.seed(123)\n\nmodel = Sequential([\n    Dense(30, activation=\"leaky_relu\"),\n    Dense(1, activation=\"exponential\")\n])\n\nmodel.compile(\"adam\", \"mse\")\n\n%time hist = model.fit( \\\n    X_train_sc, \\\n    y_train, \\\n    epochs=50, \\\n    verbose=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 26.9 s, sys: 1.64 s, total: 28.5 s\nWall time: 16.5 s\n```\n:::\n:::\n\n\n## Loss curve\n\n::: {.cell execution_count=63}\n``` {.python .cell-code}\nplt.plot(range(1, 51), hist.history[\"loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE\");\n```\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-64-output-1.png){}\n:::\n:::\n\n\n## Loss curve {data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=64}\n``` {.python .cell-code}\nplt.plot(range(2, 51), hist.history[\"loss\"][1:])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE\");\n```\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-65-output-1.png){}\n:::\n:::\n\n\n## Predictions\n\n::: {.cell execution_count=65}\n``` {.python .cell-code}\ny_pred = model.predict(X_val_sc, verbose=0)\nprint(f\"Min prediction: {y_pred.min():.2f}\")\nprint(f\"Max prediction: {y_pred.max():.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMin prediction: 0.00\nMax prediction: 17.91\n```\n:::\n:::\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=66}\n``` {.python .cell-code}\nplt.scatter(y_pred, y_val)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"True values\")\nadd_diagonal_line()\n```\n:::\n\n\n::: {.cell execution_count=67}\n``` {.python .cell-code}\nmse_train[\"Exp ANN\"] = mse(\n    y_train, model.predict(X_train_sc, verbose=0)\n)\nmse_val[\"Exp ANN\"] = mse(y_val, model.predict(X_val_sc, verbose=0))\n```\n:::\n\n\n:::\n::: column\n\n<div style=\"position: relative; top: -2em;\">\n\n::: {.cell execution_count=68}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-69-output-1.png){}\n:::\n:::\n\n\n</div>\n\n:::\n:::\n\n::: {.content-visible unless-format=\"revealjs\"}\nNow the predictions are always non-negative.\n:::\n\n## Comparing MSE (smaller is better)\n\nOn training data:\n\n::: {.cell execution_count=69}\n``` {.python .cell-code}\nmse_train\n```\n\n::: {.cell-output .cell-output-display execution_count=66}\n```\n{'Linear Regression': 0.5291948207479792,\n 'Basic ANN': 8.374372581119095,\n 'Long run ANN': 2.7147757579222658,\n 'Exp ANN': 0.31800784420349265}\n```\n:::\n:::\n\n\nOn validation data (expect _worse_, i.e. bigger):\n\n::: {.cell execution_count=70}\n``` {.python .cell-code}\nmse_val\n```\n\n::: {.cell-output .cell-output-display execution_count=67}\n```\n{'Linear Regression': 0.5059420205381367,\n 'Basic ANN': 8.391647186561524,\n 'Long run ANN': 2.7237286007300514,\n 'Exp ANN': 0.36563095992348726}\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nNote: The error on the validation set is usually higher than the training set.\n:::\n## Comparing models (train) {data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=71}\n``` {.python .cell-code}\ntrain_results = pd.DataFrame(\n    {\"Model\": mse_train.keys(), \"MSE\": mse_train.values()}\n)\ntrain_results.sort_values(\"MSE\", ascending=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=68}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>MSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Basic ANN</td>\n      <td>8.374373</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Long run ANN</td>\n      <td>2.714776</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Linear Regression</td>\n      <td>0.529195</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Exp ANN</td>\n      <td>0.318008</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Comparing models (validation) {data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=72}\n``` {.python .cell-code}\nval_results = pd.DataFrame(\n    {\"Model\": mse_val.keys(), \"MSE\": mse_val.values()}\n)\nval_results.sort_values(\"MSE\", ascending=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=69}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>MSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Basic ANN</td>\n      <td>8.391647</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Long run ANN</td>\n      <td>2.723729</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Linear Regression</td>\n      <td>0.505942</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Exp ANN</td>\n      <td>0.365631</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Early Stopping {visibility=\"uncounted\"}\n\n## Choosing when to stop training\n\n![Illustrative loss curves over time.](heaton-error-over-time.png)\n\n::: {.content-visible unless-format=\"revealjs\"}\nEarly stopping can be seen as a regularization technique to avoid overfitting. The plot shows that both training error and validation error decrease at the beginning of training process. However, after a while, validation error starts to increase while training error keeps on decreasing. This is an indication of overfitting. Overfitting leads to poor performance on the unseen data, which is seen here through the gradual increase of validation error. Early stopping can track the model's performance through the training process and stop the training at the right time.\n:::\n::: footer\nSource: Heaton (2022), [Applications of Deep Learning](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_03_4_early_stop.ipynb), Part 3.4: Early Stopping.\n:::\n\n## Try early stopping\n\nHinton calls it a \"beautiful free lunch\"\n\n::: {.cell execution_count=73}\n``` {.python .cell-code code-line-numbers=\"|1,10,13\"}\nfrom keras.callbacks import EarlyStopping                                       #<1>\n\nrandom.seed(123)                                                                #<2>\nmodel = Sequential([                                                            #<3>\n    Dense(30, activation=\"leaky_relu\"),                                               #<3>\n    Dense(1, activation=\"exponential\")                                          #<3>\n])                                                                              #<3>\nmodel.compile(\"adam\", \"mse\")                                                    #<4>\n\nes = EarlyStopping(restore_best_weights=True, patience=15)                      #<5>\n\n%time hist = model.fit(X_train_sc, y_train, epochs=1_000, \\\n    callbacks=[es], validation_data=(X_val_sc, y_val), verbose=False)           #<6>\nprint(f\"Keeping model at epoch #{len(hist.history['loss'])-10}.\")               #<7>              \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 17.1 s, sys: 1.04 s, total: 18.1 s\nWall time: 10.8 s\nKeeping model at epoch #14.\n```\n:::\n:::\n\n\n1. Imports `EarlyStopping` from `keras.callbacks`\n2. Sets the random seed\n3. Constructs the sequential model\n4. Configures the training process with optimiser and loss function\n5. Defines the early stopping object. Here, the `patience` parameter tells how many epochs the neural network has to wait without no improvement before the process stops. `patience=15` indicates that the neural network will wait for 15 epochs without any improvement before it stops training. `restore_best_weights=True` ensures that model's weights will be restored to the best model, i.e., the model we saw before 15 epochs\n6. Fits the model with early stopping object passed in\n7. Prints the outs \n\n## Loss curve\n\n::: {.cell execution_count=74}\n``` {.python .cell-code}\nplt.plot(hist.history[\"loss\"])\nplt.plot(hist.history[\"val_loss\"])\nplt.legend([\"Training\", \"Validation\"]);\n```\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-75-output-1.png){}\n:::\n:::\n\n\n## Loss curve II\n\n::: {.cell execution_count=75}\n``` {.python .cell-code}\nplt.plot(hist.history[\"loss\"])\nplt.plot(hist.history[\"val_loss\"])\nplt.ylim([0, 8])\nplt.legend([\"Training\", \"Validation\"]);\n```\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-76-output-1.png){}\n:::\n:::\n\n\n## Predictions\n\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=77}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-78-output-1.png){}\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=78}\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-79-output-1.png){}\n:::\n:::\n\n\n:::\n:::\n\n\n\n\n\n## Comparing models (validation) {data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=81}\n\n::: {.cell-output .cell-output-display execution_count=78}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>MSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Basic ANN</td>\n      <td>8.391647</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Long run ANN</td>\n      <td>2.723729</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Linear Regression</td>\n      <td>0.505942</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Early stop ANN</td>\n      <td>0.386975</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Exp ANN</td>\n      <td>0.365631</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nMSE error on the validation set has improved from the ANN model without early stopping (0.354653) to the one with early stopping (0.326440).\n:::\n\n## The test set\n\nEvaluate _only the final/selected model_ on the test set.\n\n::: {.cell execution_count=82}\n``` {.python .cell-code}\nmse(y_test, model.predict(X_test_sc, verbose=0))\n```\n\n::: {.cell-output .cell-output-display execution_count=79}\n```\n0.40260477080349\n```\n:::\n:::\n\n\n::: {.cell execution_count=83}\n``` {.python .cell-code}\nmodel.evaluate(X_test_sc, y_test, verbose=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=80}\n```\n0.40260475873947144\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\nEvaluating the model on the unseen test set provides an unbiased view on how the model will perform.\nSince we configured the model to track 'mse' as the loss function, we can simply use `model.evaluate()` function on the test set and get the same answer.\n:::\n\n## Another useful callback\n\n::: {.cell execution_count=84}\n``` {.python .cell-code}\nfrom pathlib import Path\nfrom keras.callbacks import ModelCheckpoint                                 #<1>\n\nrandom.seed(123)\nmodel = Sequential(\n    [Dense(30, activation=\"leaky_relu\"), Dense(1, activation=\"exponential\")]\n)\nmodel.compile(\"adam\", \"mse\")\nmc = ModelCheckpoint(                                                       #<2>\n    \"best-model.keras\", monitor=\"val_loss\", save_best_only=True             #<2>\n)                                                                           #<2>\nes = EarlyStopping(restore_best_weights=True, patience=5)\nhist = model.fit(                                                           #<3>\n    X_train_sc,                                                             #<3>\n    y_train,                                                                #<3>\n    epochs=100,                                                             #<3>\n    validation_split=0.1,                                                   #<3>\n    callbacks=[mc, es],                                                     #<3>\n    verbose=False,                                                          #<3>\n)\nPath(\"best-model.keras\").stat().st_size                                     #<4>\n```\n\n::: {.cell-output .cell-output-display execution_count=81}\n```\n20002\n```\n:::\n:::\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n`ModelCheckpoint` is also another useful `callback` function that can be used to save the model at some intervals during training. This is useful when training large datasets. If the training process gets interrupted at some point, last saved set of weights from model checkpoints can be used to resume the training process instead of starting from the beginning.\n:::\n\n\n\n::: {.content-hidden unless-format=\"revealjs\"}\n\n\n# Quiz {visibility=\"uncounted\"}\n\n## Critique this 💩 regression code\n\n\n\n\n\n::: {.cell execution_count=88}\n``` {.python .cell-code}\nX_train = features[:80]; X_test = features[81:]\ny_train = targets[:80]; y_test = targets[81:]\n```\n:::\n\n\n::: {.cell execution_count=89}\n``` {.python .cell-code}\nmodel = Sequential([\n   Input((2,)),\n  Dense(32, activation='relu'),\n   Dense(32, activation='relu'),\n  Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer=\"adam\", loss='mse')\nes = EarlyStopping(patience=10)\nfitted_model = model.fit(X_train, y_train, epochs=5,\n  callbacks=[es], verbose=False)\n```\n:::\n\n\n::: {.cell execution_count=90}\n``` {.python .cell-code}\ntrainMAE = model.evaluate(X_train, y_train, verbose=False)\nhist = model.fit(X_test, y_test, epochs=5,\n  callbacks=[es], verbose=False)\nhist.history[\"loss\"]\ntestMAE = model.evaluate(X_test, y_test, verbose=False)\n```\n:::\n\n\n::: {.cell execution_count=91}\n``` {.python .cell-code}\nf\"Train MAE: {testMAE:.2f} Test MAE: {trainMAE:.2f}\"\n```\n\n::: {.cell-output .cell-output-display execution_count=88}\n```\n'Train MAE: 4.82 Test MAE: 4.32'\n```\n:::\n:::\n\n\n## The data\n\n\n\n::: columns\n::: column\n\n::: {.cell execution_count=93}\n``` {.python .cell-code}\nplt.scatter(x, y, c=targets)\nplt.colorbar()\n```\n\n::: {.cell-output .cell-output-display execution_count=90}\n```\n<matplotlib.colorbar.Colorbar at 0x7f6ce5289490>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-94-output-2.png){}\n:::\n:::\n\n\n:::\n::: column\n\n::: {.cell execution_count=94}\n``` {.python .cell-code}\nplt.hist(targets, bins=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](deep-learning-keras_files/figure-revealjs/cell-95-output-1.png){}\n:::\n:::\n\n\n:::\n:::\n\n\n\n:::\n\n## Package Versions {.appendix data-visibility=\"uncounted\"}\n\n::: {.cell execution_count=95}\n``` {.python .cell-code}\nfrom watermark import watermark\nprint(watermark(python=True, packages=\"keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tf_keras\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.27.0\n\nkeras     : 3.5.0\nmatplotlib: 3.9.2\nnumpy     : 1.26.4\npandas    : 2.2.2\nseaborn   : 0.13.2\nscipy     : 1.11.0\ntorch     : 2.4.1\ntensorflow: 2.17.0\ntf_keras  : 2.17.0\n\n```\n:::\n:::\n\n\n## Glossary {.appendix data-visibility=\"uncounted\"}\n\n::: columns\n::: column\n- callbacks\n- cost/loss function\n- early stopping\n- epoch\n- Keras, Tensorflow, PyTorch\n:::\n::: column\n- matplotlib\n- targets\n- training/test split\n- validation set\n:::\n:::\n\n",
    "supporting": [
      "deep-learning-keras_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}